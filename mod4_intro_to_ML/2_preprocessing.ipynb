{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-0001",
      "metadata": {},
      "source": [
        "# Intro to Machine Learning: Preprocessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "overview-0002",
      "metadata": {},
      "source": [
        "## Overview\n",
        "Before we can train a model to predict or classify anything, we need to make sure our **data is ready to learn from**. This is what preprocessing is all about — cleaning, organizing, and transforming data so that it can be understood by algorithms.\n",
        "\n",
        "Throughout this lecture, we’ll learn **why preprocessing is important**, explore key techniques like **scaling**, **encoding**, and **dimensionality reduction**, and discuss how these steps can improve performance and interpretability.\n",
        "\n",
        "By the end, you should be able to:\n",
        "- Explain what preprocessing is and why we do it.\n",
        "- Describe how scaling and encoding work.\n",
        "- Understand what PCA (Principal Component Analysis) does.\n",
        "- Recognize when preprocessing helps a model and when it might not be necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "why-preprocess-0003",
      "metadata": {},
      "source": [
        "## Why Preprocess?\n",
        "Raw data is rarely clean or consistent. Some features might be on different scales (like income in dollars vs. age in years), and others might be categorical (like city names or shirt sizes). Without preprocessing, your model might misinterpret these differences.\n",
        "\n",
        "For example, imagine building a model to predict a student’s grade based on study hours and number of snacks eaten while studying. If you don’t adjust the scales, the feature with bigger numbers (snacks, possibly in the hundreds!) could outweigh study hours.\n",
        "\n",
        "### Common Reasons to Preprocess:\n",
        "1. **Standardization**: Ensures all numerical features are measured on similar scales.\n",
        "2. **Encoding**: Converts text categories into numbers that models can understand.\n",
        "3. **Dimensionality Reduction**: Simplifies datasets with many features without losing much information.\n",
        "\n",
        "### Discussion Question 1:\n",
        "Why might a model give inaccurate predictions if some features have much larger numbers than others?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scaling-0004",
      "metadata": {},
      "source": [
        "## Scaling Data\n",
        "Scaling is one of the most important preprocessing steps. It ensures that features measured in different units are treated fairly by algorithms.\n",
        "\n",
        "Many models — like **K-Nearest Neighbors (KNN)** and **Logistic Regression** — rely on mathematical distances or optimization processes. When one feature’s scale is much larger than another’s, it can dominate these calculations.\n",
        "\n",
        "### Two Common Scaling Methods\n",
        "1. **Standard Scaling (Z-Score Normalization)**  \n",
        "   - Formula:  \n",
        "     $$ z = \\frac{x - \\mu}{\\sigma} $$\n",
        "   - This centers the data around zero and makes the standard deviation equal to one. Useful for models that assume normally distributed data.\n",
        "\n",
        "2. **Min-Max Scaling**  \n",
        "   - Formula:  \n",
        "     $$ x' = \\frac{x - x_{min}}{x_{max} - x_{min}} $$\n",
        "   - This rescales all values to fall between 0 and 1. It’s great when you want all features to have equal influence.\n",
        "\n",
        "### Example: Predicting a Person’s Happiness\n",
        "Suppose you’re studying factors that affect happiness. You collect data on **hours of sleep** (0–12) and **monthly income** (0–10,000). If you don’t scale these features, the model may think income is far more important simply because it has larger numbers.\n",
        "\n",
        "By scaling both features, you let the model evaluate them on equal footing.\n",
        "\n",
        "### Discussion Question 2:\n",
        "When might Min-Max scaling be better than Standard scaling?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "encoding-0005",
      "metadata": {},
      "source": [
        "## Encoding Categorical Data\n",
        "Machine learning models can only understand **numbers**, not text. So we must convert any categorical (non-numeric) data into numeric form.\n",
        "\n",
        "### Two Main Types of Encoding\n",
        "1. **One-Hot Encoding**: For categories without order (like colors or city names).  \n",
        "   - Example: ‘Color’ → Red = [1, 0, 0], Blue = [0, 1, 0], Green = [0, 0, 1]\n",
        "   - Creates a new column for each category, marking presence (1) or absence (0).\n",
        "\n",
        "2. **Ordinal Encoding**: For categories with a natural order (like sizes or satisfaction levels).  \n",
        "   - Example: ‘Size’ → Small = 1, Medium = 2, Large = 3\n",
        "   - This tells the model that Large > Medium > Small.\n",
        "\n",
        "### Example: Movie Preferences\n",
        "You’re analyzing survey data about favorite movie genres and enjoyment levels. ‘Genre’ (Comedy, Drama, Action) can be One-Hot Encoded, while ‘Enjoyment’ (Low, Medium, High) should be Ordinal Encoded.\n",
        "\n",
        "### Key Note\n",
        "Using the wrong encoder can mislead your model. If you Ordinal Encode unordered data (like ‘Genre’), the model might assume Drama > Comedy just because of higher numbers.\n",
        "\n",
        "### Discussion Question 3:\n",
        "When could One-Hot Encoding become a problem if there are many possible categories?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pca-0006",
      "metadata": {},
      "source": [
        "## Dimensionality Reduction: Principal Component Analysis (PCA)\n",
        "In large datasets with many features, some information overlaps or doesn’t add much value. PCA helps reduce the number of features while keeping the most important patterns.\n",
        "\n",
        "### How PCA Works\n",
        "1. Finds directions (called **principal components**) where the data varies the most.\n",
        "2. Projects the data into a new, smaller space while preserving most of its structure.\n",
        "\n",
        "Think of PCA as summarizing a long story into a few key points — you still understand the main idea without reading every detail.\n",
        "\n",
        "### Example: Fashion Trends\n",
        "Imagine you have data on 1,000 shoppers with 20 features each (color preferences, style choices, budgets, etc.). PCA could help you reduce those 20 features down to just 2 or 3 — allowing you to plot the shoppers in 2D and see clusters like ‘casual’, ‘sporty’, and ‘formal’ styles.\n",
        "\n",
        "### When to Use PCA\n",
        "- When you have many correlated features.\n",
        "- When you want to visualize high-dimensional data.\n",
        "- When reducing computational cost matters.\n",
        "\n",
        "### But Be Careful\n",
        "PCA can make results less interpretable because new features (components) are combinations of old ones. Sometimes, simplicity comes at the cost of explainability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrapup-0007",
      "metadata": {},
      "source": [
        "## Bringing It All Together\n",
        "Let’s summarize how preprocessing fits into the broader ML workflow:\n",
        "\n",
        "1. **Collect Data** — Surveys, experiments, sensors, or public datasets.\n",
        "2. **Clean Data** — Handle missing values, fix outliers, and remove irrelevant features.\n",
        "3. **Preprocess** — Scale numerical values, encode categorical ones, and maybe reduce dimensions.\n",
        "4. **Train Model** — Use processed data for training.\n",
        "5. **Evaluate Results** — Check accuracy, precision, or other metrics.\n",
        "\n",
        "### Example Scenario\n",
        "You’re building a model to predict which products a store should restock. You have features like price, product category, weekly sales, and supplier rating. You would:\n",
        "1. Scale price and sales.\n",
        "2. Encode product category.\n",
        "3. Maybe apply PCA to simplify patterns across multiple supplier ratings.\n",
        "\n",
        "### Discussion Question 4:\n",
        "Which preprocessing step do you think would have the biggest impact in this scenario — scaling, encoding, or PCA?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary-0008",
      "metadata": {},
      "source": [
        "## Summary\n",
        "Preprocessing is how we prepare data so that it’s ready to learn.\n",
        "\n",
        "- **Scaling** puts features on similar scales.\n",
        "- **Encoding** lets models understand words and categories.\n",
        "- **PCA** simplifies complex data while keeping its structure.\n",
        "\n",
        "By applying these techniques, we help models learn more efficiently, make fairer predictions, and uncover clearer patterns.\n",
        "\n",
        "**Next Steps:** In the next lesson, we’ll see how preprocessing fits into an actual ML pipeline using real-world data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
