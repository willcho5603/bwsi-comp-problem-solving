{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617d3581",
   "metadata": {},
   "source": [
    "# Intro to ML: Preprocessing Data\n",
    "## Getting Your Data Ready for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7007066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf0b606",
   "metadata": {},
   "source": [
    "## What is Preprocessing?\n",
    "\n",
    "Imagine you're a chef preparing ingredients before cooking. You wash vegetables, chop them to similar sizes, and organize everything before you start cooking. Data preprocessing is similar - it's preparing your data before feeding it to a machine learning model!\n",
    "\n",
    "**Real-world analogy:** Before Spotify recommends songs to you, it needs to process data about:\n",
    "- Song duration (measured in seconds)\n",
    "- Popularity (measured 0-100)\n",
    "- Genre (text like \"pop\", \"rock\", \"hip-hop\")\n",
    "- Release year (1950-2024)\n",
    "\n",
    "These features are all in different formats and scales - that's where preprocessing comes in!\n",
    "\n",
    "### When and Why Preprocess?\n",
    "- **Data cleaning**: Remove errors, handle missing values, standardize formats\n",
    "- **Better model performance**: Many ML algorithms work better with preprocessed data\n",
    "- **Faster training**: Properly scaled data can help models learn more efficiently\n",
    "- **Fair comparisons**: Ensure no feature dominates just because of its scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro_question",
   "metadata": {},
   "source": [
    "### Q: Think about a dataset of student information. What types of data might need preprocessing?\n",
    "Consider: grades (0-100), attendance (days), favorite subject (text), height (cm), time spent studying (minutes/week)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro_answer",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f87275",
   "metadata": {},
   "source": [
    "## Part 1: Scaling (Normalization)\n",
    "\n",
    "### The Problem: Different Scales\n",
    "\n",
    "Imagine you're building a model to predict if someone will like a movie based on:\n",
    "- **Budget**: \\$1,000,000 to \\$300,000,000 (huge range!)\n",
    "- **Rating**: 1 to 10 (small range)\n",
    "- **Runtime**: 80 to 180 minutes (medium range)\n",
    "\n",
    "Without scaling, the budget would dominate the model just because the numbers are bigger!\n",
    "\n",
    "### Two Common Scaling Methods:\n",
    "\n",
    "**1. Min-Max Scaling (Normalization)**\n",
    "- Squishes all values between 0 and 1\n",
    "- Formula: $x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "- **When to use**: When you want all features in the same range and there are no extreme outliers\n",
    "\n",
    "**2. Standard Scaling (Standardization)**\n",
    "- Centers data around 0 with standard deviation of 1\n",
    "- Formula: $x_{scaled} = \\frac{x - \\mu}{\\sigma}$ where $\\mu$ is mean and $\\sigma$ is standard deviation\n",
    "- **When to use**: When your data has outliers or follows a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling_example_setup",
   "metadata": {},
   "source": [
    "### Example: Video Game Stats\n",
    "Let's say we're analyzing video games with these features:\n",
    "- **Play time**: How many hours to complete (10-200 hours)\n",
    "- **Price**: Cost in dollars (\\$5-\\$70)\n",
    "- **User rating**: Average rating (1-5 stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scaling_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample video game data\n",
    "np.random.seed(42)\n",
    "n_games = 50\n",
    "\n",
    "play_time = np.random.uniform(10, 200, n_games)  # hours\n",
    "price = np.random.uniform(5, 70, n_games)        # dollars\n",
    "rating = np.random.uniform(1, 5, n_games)        # stars\n",
    "\n",
    "# Combine into a dataset\n",
    "data = np.column_stack([play_time, price, rating])\n",
    "\n",
    "print(\"Original Data Statistics:\")\n",
    "print(f\"Play Time - Min: {play_time.min():.1f}, Max: {play_time.max():.1f}, Mean: {play_time.mean():.1f}\")\n",
    "print(f\"Price - Min: ${price.min():.1f}, Max: ${price.max():.1f}, Mean: ${price.mean():.1f}\")\n",
    "print(f\"Rating - Min: {rating.min():.2f}, Max: {rating.max():.2f}, Mean: {rating.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scaling_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply different scaling methods\n",
    "minmax_scaler = MinMaxScaler()\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "data_minmax = minmax_scaler.fit_transform(data)\n",
    "data_standard = standard_scaler.fit_transform(data)\n",
    "\n",
    "# Visualize the scaling effects\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "feature_names = ['Play Time', 'Price', 'Rating']\n",
    "\n",
    "for i, (ax, name) in enumerate(zip(axes, feature_names)):\n",
    "    ax.scatter(range(n_games), data[:, i], alpha=0.6, label='Original', s=50)\n",
    "    ax.scatter(range(n_games), data_minmax[:, i], alpha=0.6, label='Min-Max', s=50)\n",
    "    ax.scatter(range(n_games), data_standard[:, i], alpha=0.6, label='Standard', s=50)\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.set_xlabel('Game Index')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAfter Min-Max Scaling (all values 0-1):\")\n",
    "print(f\"Play Time - Min: {data_minmax[:, 0].min():.3f}, Max: {data_minmax[:, 0].max():.3f}\")\n",
    "print(f\"Price - Min: {data_minmax[:, 1].min():.3f}, Max: {data_minmax[:, 1].max():.3f}\")\n",
    "print(f\"Rating - Min: {data_minmax[:, 2].min():.3f}, Max: {data_minmax[:, 2].max():.3f}\")\n",
    "\n",
    "print(\"\\nAfter Standard Scaling (mean=0, std=1):\")\n",
    "print(f\"Play Time - Mean: {data_standard[:, 0].mean():.3f}, Std: {data_standard[:, 0].std():.3f}\")\n",
    "print(f\"Price - Mean: {data_standard[:, 1].mean():.3f}, Std: {data_standard[:, 1].std():.3f}\")\n",
    "print(f\"Rating - Mean: {data_standard[:, 2].mean():.3f}, Std: {data_standard[:, 2].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling_question",
   "metadata": {},
   "source": [
    "### Q: Looking at the graphs above, answer these questions:\n",
    "a. What do you notice about the range of values after Min-Max scaling?\n",
    "\n",
    "b. What do you notice about the shape/pattern of the data points - does scaling change the relative relationships between points?\n",
    "\n",
    "c. Which scaling method would you choose if you had a dataset with a few extreme outliers? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scaling_answer",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3655400",
   "metadata": {},
   "source": [
    "## Part 2: Encoding Categorical Variables\n",
    "\n",
    "### The Problem: Computers Only Understand Numbers!\n",
    "\n",
    "Machine learning models can't directly work with text. If you have categories like:\n",
    "- **T-shirt sizes**: Small, Medium, Large\n",
    "- **Music genres**: Pop, Rock, Jazz, Hip-Hop\n",
    "- **Days of week**: Monday, Tuesday, Wednesday...\n",
    "\n",
    "We need to convert these into numbers!\n",
    "\n",
    "### Two Main Encoding Methods:\n",
    "\n",
    "**1. Label Encoding (Ordinal)**\n",
    "- Assigns a unique integer to each category\n",
    "- Example: [\"Small\", \"Medium\", \"Large\"] → [0, 1, 2]\n",
    "- **When to use**: When categories have a natural order (like sizes, ratings, grades)\n",
    "- **Warning**: Don't use for categories without order! (The model might think \"Rock\" is twice as much as \"Pop\" if encoded as 1 and 2)\n",
    "\n",
    "**2. One-Hot Encoding**\n",
    "- Creates a new binary column for each category\n",
    "- Example: [\"Pop\", \"Rock\", \"Jazz\"] becomes:\n",
    "  - Pop: [1, 0, 0]\n",
    "  - Rock: [0, 1, 0]\n",
    "  - Jazz: [0, 0, 1]\n",
    "- **When to use**: When categories have NO natural order (like genres, colors, countries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding_example_setup",
   "metadata": {},
   "source": [
    "### Example: Social Media App Usage\n",
    "Let's analyze how different people use social media based on their preferences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encoding_example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample social media data\n",
    "user_data = pd.DataFrame({\n",
    "    'User': ['Alice', 'Bob', 'Carol', 'David', 'Eve', 'Frank', 'Grace', 'Henry'],\n",
    "    'Experience_Level': ['Beginner', 'Advanced', 'Intermediate', 'Beginner', 'Advanced', 'Intermediate', 'Advanced', 'Beginner'],\n",
    "    'Favorite_Platform': ['Instagram', 'TikTok', 'Instagram', 'Twitter', 'TikTok', 'Instagram', 'Twitter', 'TikTok'],\n",
    "    'Daily_Hours': [2, 5, 3, 1, 6, 4, 2, 3]\n",
    "})\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(user_data)\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "label_encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding for Experience Level (has natural order!)\n",
    "experience_mapping = {'Beginner': 0, 'Intermediate': 1, 'Advanced': 2}\n",
    "user_data['Experience_Encoded'] = user_data['Experience_Level'].map(experience_mapping)\n",
    "\n",
    "print(\"\\nLabel Encoding for Experience Level:\")\n",
    "print(user_data[['User', 'Experience_Level', 'Experience_Encoded']])\n",
    "print(\"\\nThis makes sense because: Beginner < Intermediate < Advanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "onehot_encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding for Favorite Platform (no natural order!)\n",
    "platform_onehot = pd.get_dummies(user_data['Favorite_Platform'], prefix='Platform')\n",
    "user_data_encoded = pd.concat([user_data, platform_onehot], axis=1)\n",
    "\n",
    "print(\"\\nOne-Hot Encoding for Favorite Platform:\")\n",
    "print(user_data_encoded[['User', 'Favorite_Platform', 'Platform_Instagram', 'Platform_TikTok', 'Platform_Twitter']])\n",
    "print(\"\\nThis makes sense because: Instagram, TikTok, and Twitter have no ranking!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encoding_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the encoding\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Label Encoding\n",
    "exp_counts = user_data['Experience_Level'].value_counts()\n",
    "colors_exp = ['#FF6B6B', '#FFA500', '#4ECDC4']\n",
    "axes[0].bar(exp_counts.index, exp_counts.values, color=colors_exp, edgecolor='black', linewidth=2)\n",
    "axes[0].set_title('Experience Level (Label Encoding)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Users', fontsize=12)\n",
    "axes[0].set_xlabel('Category (Beginner=0, Intermediate=1, Advanced=2)', fontsize=12)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: One-Hot Encoding\n",
    "platform_counts = user_data['Favorite_Platform'].value_counts()\n",
    "colors_platform = ['#9B59B6', '#E74C3C', '#3498DB']\n",
    "axes[1].bar(platform_counts.index, platform_counts.values, color=colors_platform, edgecolor='black', linewidth=2)\n",
    "axes[1].set_title('Favorite Platform (One-Hot Encoding)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Number of Users', fontsize=12)\n",
    "axes[1].set_xlabel('Each platform gets its own column (0 or 1)', fontsize=12)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding_question",
   "metadata": {},
   "source": [
    "### Q: Consider a dataset of movies with these features:\n",
    "- **Rating**: G, PG, PG-13, R (age-based restrictions)\n",
    "- **Genre**: Action, Comedy, Drama, Horror, Sci-Fi\n",
    "- **Review Score**: Poor, Fair, Good, Excellent\n",
    "\n",
    "For each feature, should you use Label Encoding or One-Hot Encoding? Explain your reasoning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encoding_answer",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practice_encoding",
   "metadata": {},
   "source": [
    "### Q: Practice Time! Complete the code below to encode the fitness app data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encoding_practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice dataset: Fitness App Users\n",
    "fitness_data = pd.DataFrame({\n",
    "    'User_ID': [1, 2, 3, 4, 5],\n",
    "    'Fitness_Level': ['Beginner', 'Expert', 'Intermediate', 'Expert', 'Beginner'],\n",
    "    'Workout_Type': ['Yoga', 'Running', 'Swimming', 'Yoga', 'Running'],\n",
    "    'Weekly_Hours': [3, 8, 5, 7, 4]\n",
    "})\n",
    "\n",
    "print(\"Original Fitness Data:\")\n",
    "print(fitness_data)\n",
    "\n",
    "# TODO: Create label encoding for Fitness_Level (Beginner=0, Intermediate=1, Expert=2)\n",
    "# fitness_data['Fitness_Level_Encoded'] = ???\n",
    "\n",
    "# TODO: Create one-hot encoding for Workout_Type\n",
    "# workout_onehot = ???\n",
    "\n",
    "print(\"\\nYour encoded data should appear here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc14180b",
   "metadata": {},
   "source": [
    "## Part 3: Principal Component Analysis (PCA)\n",
    "\n",
    "### The Problem: Too Many Features!\n",
    "\n",
    "Imagine you're trying to describe your classmates. You could use:\n",
    "- Height, weight, age, grade level\n",
    "- Test scores in 10 different subjects\n",
    "- Hours spent on 5 different hobbies\n",
    "- Social media usage across 8 platforms\n",
    "\n",
    "That's 27+ features! But many of these are probably related:\n",
    "- If someone is tall, they might also be heavy\n",
    "- If someone is good at Math, they might be good at Physics\n",
    "- If someone uses Instagram a lot, they might also use TikTok a lot\n",
    "\n",
    "**PCA helps us combine related features into a smaller number of \"principal components\"!**\n",
    "\n",
    "### What is PCA?\n",
    "- **Principal Component Analysis** finds the most important patterns in your data\n",
    "- It creates new features that capture the most variation\n",
    "- Like taking a 3D object and finding the best angle to photograph it in 2D!\n",
    "\n",
    "### Why Use PCA?\n",
    "1. **Reduce complexity**: Easier to visualize and understand\n",
    "2. **Remove redundancy**: Eliminate correlated features\n",
    "3. **Faster training**: Fewer features = faster models\n",
    "4. **Avoid overfitting**: Too many features can make models memorize instead of learn\n",
    "\n",
    "### Real-World Example:\n",
    "Netflix doesn't store your preference for every single movie (thousands of features). Instead, it might use PCA to find that you're:\n",
    "- 80% \"Action Lover\"\n",
    "- 60% \"Comedy Fan\"\n",
    "- 20% \"Documentary Watcher\"\n",
    "\n",
    "These 3 components capture most of your viewing patterns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pca_example_setup",
   "metadata": {},
   "source": [
    "### Example: Student Academic Performance\n",
    "Let's analyze student grades across multiple subjects and see if we can find underlying patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_data_generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate student grade data\n",
    "np.random.seed(42)\n",
    "n_students = 100\n",
    "\n",
    "# Create correlated grades - students good at one subject tend to be good at related subjects\n",
    "# STEM skills (Math, Physics, Computer Science)\n",
    "stem_ability = np.random.randn(n_students) * 15 + 75\n",
    "math_grades = stem_ability + np.random.randn(n_students) * 5\n",
    "physics_grades = stem_ability + np.random.randn(n_students) * 7\n",
    "cs_grades = stem_ability + np.random.randn(n_students) * 6\n",
    "\n",
    "# Humanities skills (English, History, Art)\n",
    "humanities_ability = np.random.randn(n_students) * 12 + 70\n",
    "english_grades = humanities_ability + np.random.randn(n_students) * 6\n",
    "history_grades = humanities_ability + np.random.randn(n_students) * 8\n",
    "art_grades = humanities_ability + np.random.randn(n_students) * 7\n",
    "\n",
    "# Combine all grades\n",
    "grades_data = np.column_stack([\n",
    "    math_grades, physics_grades, cs_grades,\n",
    "    english_grades, history_grades, art_grades\n",
    "])\n",
    "\n",
    "# Clip grades to 0-100 range\n",
    "grades_data = np.clip(grades_data, 0, 100)\n",
    "\n",
    "subject_names = ['Math', 'Physics', 'CS', 'English', 'History', 'Art']\n",
    "\n",
    "# Create DataFrame\n",
    "grades_df = pd.DataFrame(grades_data, columns=subject_names)\n",
    "print(\"Sample of Student Grades:\")\n",
    "print(grades_df.head(10))\n",
    "print(f\"\\nDataset shape: {grades_df.shape[0]} students × {grades_df.shape[1]} subjects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlations between subjects\n",
    "correlation_matrix = grades_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "im = plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "plt.colorbar(im, label='Correlation Coefficient')\n",
    "plt.xticks(range(len(subject_names)), subject_names, rotation=45)\n",
    "plt.yticks(range(len(subject_names)), subject_names)\n",
    "plt.title('How Related Are These Subjects?\\n(Brighter = More Related)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add correlation values\n",
    "for i in range(len(subject_names)):\n",
    "    for j in range(len(subject_names)):\n",
    "        text = plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWhat does this mean?\")\n",
    "print(\"- Values close to 1: Students who do well in one subject tend to do well in the other\")\n",
    "print(\"- Values close to 0: The subjects are independent\")\n",
    "print(\"- Values close to -1: Students who do well in one tend to do poorly in the other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_application",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce from 6 subjects to 2 principal components\n",
    "# First, we need to standardize the data\n",
    "scaler = StandardScaler()\n",
    "grades_scaled = scaler.fit_transform(grades_data)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "grades_pca = pca.fit_transform(grades_scaled)\n",
    "\n",
    "print(\"PCA Results:\")\n",
    "print(f\"Original data: {grades_data.shape[1]} features (subjects)\")\n",
    "print(f\"After PCA: {grades_pca.shape[1]} features (principal components)\")\n",
    "print(f\"\\nVariance explained by each component:\")\n",
    "print(f\"Component 1: {pca.explained_variance_ratio_[0]*100:.1f}%\")\n",
    "print(f\"Component 2: {pca.explained_variance_ratio_[1]*100:.1f}%\")\n",
    "print(f\"Total variance captured: {sum(pca.explained_variance_ratio_)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dimensionality reduction\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Original data (showing 2 subjects as example)\n",
    "scatter1 = axes[0].scatter(grades_data[:, 0], grades_data[:, 1], \n",
    "                          c=grades_data.mean(axis=1), cmap='viridis', \n",
    "                          s=100, alpha=0.6, edgecolors='black')\n",
    "axes[0].set_xlabel('Math Grades', fontsize=12)\n",
    "axes[0].set_ylabel('Physics Grades', fontsize=12)\n",
    "axes[0].set_title('Original Data (2 out of 6 subjects shown)', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Average Grade')\n",
    "\n",
    "# Plot 2: PCA-transformed data\n",
    "scatter2 = axes[1].scatter(grades_pca[:, 0], grades_pca[:, 1], \n",
    "                          c=grades_data.mean(axis=1), cmap='viridis',\n",
    "                          s=100, alpha=0.6, edgecolors='black')\n",
    "axes[1].set_xlabel('Principal Component 1 (\"STEM Ability\")', fontsize=12)\n",
    "axes[1].set_ylabel('Principal Component 2 (\"Humanities Ability\")', fontsize=12)\n",
    "axes[1].set_title('After PCA (All 6 subjects compressed into 2 components)', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter2, ax=axes[1], label='Average Grade')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca_components",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show what each component represents\n",
    "components_df = pd.DataFrame(\n",
