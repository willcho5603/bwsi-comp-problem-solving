{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381dd255",
   "metadata": {},
   "source": [
    "# Deep Learning: Types of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a20bf",
   "metadata": {},
   "source": [
    "Please refer to this quick reference of the different types [here](https://www.geeksforgeeks.org/deep-learning/types-of-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b43167",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "\n",
    "Before we get into the weeds of specific types of deep learning, we should briefly define the components that go into a neural network.\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20250519152928720101/bhu.webp\" width = \"500\">\n",
    "</p>\n",
    "\n",
    "### Weights\n",
    "\n",
    "### Bias\n",
    "\n",
    "### Activation Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3076894d",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20250924105551796210/2.webp\" width = \"500\">\n",
    "</p>\n",
    "\n",
    "FNNs can be depicted by **directed acyclic graphs**. This is just a fancy way of saying they are unidirectional and have no loops!\n",
    "\n",
    "They are the most basic kind of neural network which makes them great for simple tasks, particularly with tabular data, and no sequential dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668b0469",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20250924160202277839/23.webp\" width = \"500\">\n",
    "</p>\n",
    "\n",
    "Generally speaking, CNNs are mostly used for image or video based tasks, but they are well suited for any kind of gridded input.\n",
    "\n",
    "\n",
    "Use the in depth reference [here](https://www.geeksforgeeks.org/machine-learning/introduction-convolution-neural-network/) to fill out the following:\n",
    "1. Define the layers -- input, convolutional, activation, pooling, flattening, fully connected, output\n",
    "2. Show the impact of each step on an example image (i.e. show the intermediate outputs between layers)\n",
    "3. Comment on the usefulness of each step in a basic CNN architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29cfeb6",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20250523171309383561/recurrent_neural_network.webp\" width = \"500\">\n",
    "</p>\n",
    "\n",
    "RNNs can capture time dependencies by \"remembering\" information from previous steps, thanks to their **recurrent units**.\n",
    "\n",
    "In depth reference [here](https://www.geeksforgeeks.org/machine-learning/introduction-to-recurrent-neural-network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60efbbab",
   "metadata": {},
   "source": [
    "## Transformer Networks\n",
    "- Wikipedia overview [here](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture))\n",
    "- All ChatGPT models are transformer LLMs! \n",
    "- Helpful overview [here](https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/)\n",
    "- More on self-attention [here](https://www.geeksforgeeks.org/nlp/self-attention-in-nlp/)\n",
    "\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20250924111849816889/encoder_decoder_image.webp\" width = \"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cdf722",
   "metadata": {},
   "source": [
    "## Summary ðŸ“š\n",
    "- Neural networks are *much* more complicated on average than the algorithms we've seen thus far\n",
    "- Neural networks leverage **layers** to extract complex relationships\n",
    "- Feature extraction happens within the network itself, but your inputs may still require preprocessing!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
