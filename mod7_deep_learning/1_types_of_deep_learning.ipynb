{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "381dd255",
   "metadata": {},
   "source": [
    "# Deep Learning: Types of Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c4dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceptron_intro",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "\n",
    "The **Perceptron** is the simplest type of artificial neural network. It is the fundamental building block of deep learning.\n",
    "\n",
    "Think of it as a single voter deciding 'Yes' or 'No' based on weighted evidence. It takes several inputs, multiplies them by **weights**, sums them up, adds a **bias**, and passes the result through an **activation function** (like a step function) to produce an output.\n",
    "\n",
    "Mathematically: $y = f(\\sum (weight \\cdot input) + bias)$\n",
    "\n",
    "### Linear vs. Affine\n",
    "Strictly speaking, a **linear** function must pass through the origin (0,0). It obeys $f(cx+y) = cf(x) + f(y)$.\n",
    "However, most real-world decision boundaries don't pass exactly through the origin.\n",
    "\n",
    "This is why we add the **bias** term. It shifts the decision boundary away from the origin, making it an **affine** function.\n",
    "*   **Linear**: $y = w \\cdot x$ (Must go through origin)\n",
    "*   **Affine**: $y = w \\cdot x + b$ (Can be shifted)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/funny_cnn.png\" width=\"500\">\n",
    "    <br>\n",
    "    <em>Chihuahua or Muffin?</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "**Lifting Trick**: We can treat an affine function as linear by adding a \"1\" to our input vector ($x_{n+1} = 1$) and treating the bias as just another weight ($w_{n+1}$).\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/image1.png\" width=\"500\">\n",
    "    <br>\n",
    "    <em>Linear Decision Boundary</em>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceptron_algorithm",
   "metadata": {},
   "source": [
    "### Perceptron Algorithm Steps\n",
    "\n",
    "Given a set of data $S = \\{(x_1, y_1), ..., (x_n, y_n)\\}$:\n",
    "\n",
    "1.  **Initialize** $w_0 = 0$.\n",
    "2.  **Iterate** For $t=1, 2, ..., T$:\n",
    "3.  **Check** If there exists some $(x_i, y_i)$ that is classified incorrectly, in other words, $y_i(w_{t-1} \\cdot x_i) \\le 0$:\n",
    "4.  **Update** Set the next $w_t = w_{t-1} + y_i x_i$.\n",
    "5.  **Terminate** when no more data is incorrectly classified.\n",
    "\n",
    "**Runtime**: $O(n \\cdot d \\cdot T)$ where $d$ is the dimension of each data-point $x_i$ and $T$ is the maximum number of steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceptron_termination",
   "metadata": {},
   "source": [
    "### Termination of the Perceptron Algorithm\n",
    "\n",
    "The perceptron algorithm is only guaranteed to **terminate** if the data is linearly separable.\n",
    "\n",
    "We know an optimal separator $w^*$ exists separating the data $S$ with some margin $m$. (Normalize $||w^*||=1$.)\n",
    "As we have finite data, we can bound it within some radius $R$ such that $||x_i|| \\le R$ for all $x_i$.\n",
    "The perceptron algorithm will take at most $T=(R/m)^2$ steps. By steps, we mean number of times the algorithm finds a misclassified datapoint.\n",
    "\n",
    "#### Proof of Perceptron Termination\n",
    "\n",
    "1. We bound $w_t \\cdot w^* = (w_{t-1} + y_i x_i) \\cdot w^* \\ge w_{t-1} \\cdot w^* + m$.\n",
    "2. $||w_t||^2 = ||w_{t-1} + y_i x_i||^2 = ||w_{t-1}||^2 + 2y_i(w_{t-1} \\cdot x_i) + ||y_i x_i||^2$.\n",
    "\n",
    "Since a mistake occurred, $y_i(w_{t-1} \\cdot x_i) \\le 0$.\n",
    "Thus, $||w_t||^2 \\le ||w_{t-1}||^2 + R^2$.\n",
    "\n",
    "Since $w_0=0$, after $t=T$ iterations, $w_t \\cdot w^* \\ge Tm$.\n",
    "\n",
    "By **Cauchy-Schwarz**,\n",
    "$w_t \\cdot w^* \\le ||w_t|| \\cdot ||w^*|| \\le \\sqrt{T}R$.\n",
    "\n",
    "Thus, $Tm \\le \\sqrt{T}R \\implies \\sqrt{T} \\le \\frac{R}{m} \\implies T \\le (\\frac{R}{m})^2$.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/image8.gif\" width=\"500\">\n",
    "    <br>\n",
    "    <em>Visualizing Convergence (Termination)</em>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa11b95",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transition_to_nn",
   "metadata": {},
   "source": [
    "## From Perceptrons to Neural Networks\n",
    "\n",
    "### The Limitation: Linearity\n",
    "Perceptrons are powerful, but they have a major flaw: they are **linear classifiers**. This means they can only separate data that can be split by a straight line (or plane).\n",
    "\n",
    "A famous example where they fail is the **XOR problem**. You cannot draw a single straight line to separate (0,0) and (1,1) from (0,1) and (1,0).\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/image6.png\" width=\"500\">\n",
    "    <br>\n",
    "    <em>Linearization of Non-Linear Data</em>\n",
    "</p>\n",
    "\n",
    "### The Solution: Neural Networks\n",
    "To solve complex, non-linear problems, we need teamwork! By combining multiple perceptrons into **layers** and using **non-linear activation functions**, we create a **Neural Network**.\n",
    "\n",
    "This allows the network to learn complex curves and patterns, not just straight lines.\n",
    "\n",
    "### The Power of Non-Linearity: Linearization\n",
    "How do we solve non-linear problems? We can transform the space!\n",
    "\n",
    "**Linearization of New Space**: If we transform our inputs (e.g., $z = x^2$), a complex curved boundary in the original space ($X$) becomes a simple straight line in the new space ($Z$).\n",
    "\n",
    "Example:\n",
    "-   Original Circle: $g(X) = (x_1 - c_1)^2 + (x_2 - c_2)^2$\n",
    "-   Transformation: Let $z_1 = (x_1 - c_1)^2$ and $z_2 = (x_2 - c_2)^2$\n",
    "-   New Linear Boundary: $g(Z) = z_1 + z_2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4acd88f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63790534",
   "metadata": {},
   "source": [
    "## Terminology\n",
    "\n",
    "Before we get into the weeds of specific types of deep learning, we should briefly define the components that go into a neural network.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/image10.png\" width=\"500\">\n",
    "    <br>\n",
    "    <em>Anatomy of a Single Neuron</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "### Weights\n",
    "**Weights represent importance.**\n",
    "Think of them as the 'strength' of the connection between neurons. If a feature is very important for the decision (e.g., 'has wheels' for detecting a car), it will have a large weight.\n",
    "\n",
    "Geometrically, the weight vector $w$ determines the **orientation** of the decision boundary.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/image4.png\" width=\"500\">\n",
    "    <br>\n",
    "    <em>The Weight Vector is Normal to the Boundary</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "### Bias\n",
    "**Bias represents the threshold.**\n",
    "It allows the activation function to be shifted to the left or right. Without bias, a neuron would always trigger at zero input. Bias lets the neuron say, \"I only fire if the input is greater than 5.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47686472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A linear layer contains both weights (w) and bias (b)\n",
    "layer = nn.Linear(in_features=10, out_features=5, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26377713",
   "metadata": {},
   "source": [
    "\n",
    "### Activation Functions\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/image10.png\" width=\"500\">\n",
    "    <br>\n",
    "    <em>A Single Neuron with Activation</em>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/image9.png\" width=\"500\">\n",
    "    <br>\n",
    "    <em>Common Activation Functions</em>\n",
    "</p>\n",
    "\n",
    "### Key Idea ðŸ’¡\n",
    "**Activation functions introduce non-linearity**\n",
    "They decide whether a neuron should 'fire' or not. Without them, a neural network would just be one big linear regression model, no matter how many layers you stack!\n",
    "\n",
    "Common examples:\n",
    "- **Sigmoid**: S-shape, squashes output between 0 and 1 (like a probability).\n",
    "- **ReLU (Rectified Linear Unit)**: If positive, keep it; if negative, set to zero. Simple but very effective.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Example:\n",
    "```python\n",
    "output = F.relu(input_tensor)        # ReLU\n",
    "output = torch.sigmoid(input_tensor) # Sigmoid\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-5345428",
   "metadata": {},
   "source": [
    "\n",
    "### Layers\n",
    "**Layers organize neurons.**\n",
    "Neurons are arranged in layers. The first layer is the **Input Layer**, the last is the **Output Layer**, and everything in between is a **Hidden Layer**.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/998px-Colored_neural_network.svg.png\" height=\"300\" style=\"background-color:white;\">\n",
    "    <br>\n",
    "    <em>Layers of a Neural Network</em>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e9db06",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fnn_expanded",
   "metadata": {},
   "source": [
    "## Feedforward Neural Networks (FNNs)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/fnn_detailed.png\" width=\"500\">\n",
    "    <br>\n",
    "    <em>Detailed View of a Feedforward Network</em>\n",
    "</p>\n",
    "\n",
    "FNNs are the simplest type of deep neural network. In an FNN, information moves in only one directionâ€”forwardâ€”from the input nodes, through the hidden nodes (if any), and to the output nodes. There are no cycles or loops in the network.\n",
    "\n",
    "### How it Works\n",
    "1.  **Input Layer**: Receives the raw data (e.g., pixels of an image, features of a house).\n",
    "2.  **Hidden Layers**: The \"magic\" happens here. Each neuron in a hidden layer takes a weighted sum of the previous layer's outputs, adds a bias, and applies an activation function.\n",
    "    -   Mathematically: $h = f(W_1 x + b_1)$\n",
    "3.  **Output Layer**: Produces the final prediction (e.g., probability of \"Cat\").\n",
    "    -   Mathematically: $y = f(W_2 h + b_2)$\n",
    "\n",
    "### Why Hidden Layers?\n",
    "Hidden layers allow the network to learn **intermediate representations**. For example, if the input is pixels:\n",
    "-   Layer 1 might learn to detect edges.\n",
    "-   Layer 2 might combine edges to detect shapes (circles, squares).\n",
    "-   Layer 3 might combine shapes to detect objects (wheels, eyes).\n",
    "\n",
    "\n",
    "\n",
    "### The Universal Approximation Theorem\n",
    "A feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of $\\mathbb{R}^n$, under mild assumptions on the activation function.\n",
    "\n",
    "**Translation**: In simple terms, a neural network with enough neurons can learn to represent *any* function (curve) you can draw. It's a universal function approximator! This is why we moved from Perceptrons (limited to lines) to FNNs.\n",
    "\n",
    "**PyTorch Connection:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fnn_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple FNN with one hidden layer\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 128),   # Input to Hidden (784 pixels -> 128 features)\n",
    "    nn.ReLU(),             # Activation\n",
    "    nn.Linear(128, 10)     # Hidden to Output (10 classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn_intro",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20250924160202277839/23.webp\" width = \"500\">\n",
    "</p>\n",
    "\n",
    "CNNs are the superstars of **image recognition**. While FNNs treat every pixel as independent, CNNs understand spatial structure (like knowing that an eye is usually next to a nose).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn_how_it_works",
   "metadata": {},
   "source": [
    "### How it Works: The Mechanics\n",
    "\n",
    "1.  **Convolution (The Filter)**: A small matrix (kernel) slides over the image, performing element-wise multiplication and summation. This detects features like edges or curves.\n",
    "    -   Mathematically: $(I * K)(i, j) = \\sum_m \\sum_n I(i+m, j+n) K(m, n)$\n",
    "    -   **Stride**: How many pixels the filter moves at a time.\n",
    "    -   **Padding**: Adding zeros around the border to keep the image size constant.\n",
    "2.  **Activation (ReLU)**: Turns negative values to zero (introducing non-linearity).\n",
    "3.  **Pooling (Downsampling)**: Reduces the size of the feature map (e.g., taking the maximum value in a 2x2 grid). This makes the network computationally efficient and robust to small shifts in the image.\n",
    "4.  **Flattening & Fully Connected**: The final 2D feature maps are flattened into a 1D vector and fed into a standard FNN for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn_intuition",
   "metadata": {},
   "source": [
    "### Intuition: The Assembly Line\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"img/linear_vs_affine.png\" width=\"500\">\n",
    "    <br>\n",
    "    <em>Why we need CNNs: Linear vs. Affine</em>\n",
    "</p>\n",
    "\n",
    "Imagine a factory assembly line analyzing a car:\n",
    "1.  **Convolutional Layers (The Specialists)**: These are like workers scanning the object with specific tools (filters).\n",
    "    -   Early layers detect simple lines and edges.\n",
    "    -   Middle layers detect shapes (circles, squares).\n",
    "    -   Later layers detect complex objects (wheels, headlights).\n",
    "2.  **Pooling Layers (The Summarizers)**: These workers simplify the report. If a 'wheel' was found in the top-left, they just note 'wheel present' without recording its exact millimeter position. This makes the network faster and more robust.\n",
    "3.  **Fully Connected Layers (The Decision Makers)**: They take the final summary ('2 wheels', 'handlebars', 'frame') and make the final classification: 'This is a bicycle'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-8371908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 input channels (RGB), 16 output channels (features), 3x3 filter\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa3afd9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-8611122",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNNs)\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20250523171309383561/recurrent_neural_network.webp\" width = \"500\">\n",
    "</p>\n",
    "\n",
    "Standard networks have no memory of the past. **RNNs have memory.** They are designed for **sequential data** like text, speech, or stock prices.\n",
    "\n",
    "\n",
    "### How it Works: The Hidden State\n",
    "Unlike FNNs, RNNs have a **Hidden State** ($h_t$) that acts as memory. At each time step $t$:\n",
    "1.  The network takes the **current input** ($x_t$) AND the **previous hidden state** ($h_{t-1}$).\n",
    "2.  It calculates the **new hidden state** ($h_t$).\n",
    "3.  It produces an **output** ($y_t$).\n",
    "\n",
    "Mathematically: $h_t = \\tanh(W x_t + U h_{t-1} + b)$\n",
    "\n",
    "**The Challenge**: Standard RNNs suffer from **Vanishing Gradients**, meaning they forget information from long ago. This led to advanced architectures like **LSTMs** (Long Short-Term Memory) and **GRUs**.\n",
    "\n",
    "\n",
    "### Intuition: Reading a Sentence\n",
    "Imagine reading the sentence: *'I grew up in France... I speak fluent ____'.*\n",
    "\n",
    "To fill in the blank with 'French', you need to remember the word 'France' from the beginning of the sentence. A standard FNN might look at 'fluent' and guess 'English' or 'Spanish' randomly.\n",
    "\n",
    "An RNN processes words one by one, passing a **hidden state** (memory) from the previous step to the current one. It 'remembers' the context of 'France' while processing 'fluent'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10752066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input size 10, Hidden memory size 20\n",
    "rnn_layer = nn.RNN(input_size=10, hidden_size=20, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6feac",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transformer_expanded",
   "metadata": {},
   "source": [
    "## Transformer Networks\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20250924111849816889/encoder_decoder_image.webp\" width = \"500\">\n",
    "</p>\n",
    "\n",
    "Transformers are the modern evolution of NLP (Natural Language Processing). They power models like **ChatGPT** and **BERT**.\n",
    "\n",
    "### Intuition: Attention Mechanism\n",
    "RNNs are great, but they get 'tired' reading long books. They often forget the beginning of a paragraph by the time they reach the end.\n",
    "\n",
    "**Transformers don't read sequentially.** They look at the entire sentence at once and use **Self-Attention** to understand context.\n",
    "\n",
    "Think of the sentence: *'The animal didn't cross the street because **it** was too tired.'*\n",
    "When the model processes the word **'it'**, the Attention mechanism highlights **'animal'** heavily, understanding that 'it' refers to the animal, not the street.\n",
    "\n",
    "- Wikipedia overview [here](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture))\n",
    "- All ChatGPT models are transformer LLMs!\n",
    "- Helpful overview [here](https://www.geeksforgeeks.org/machine-learning/getting-started-with-transformers/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a4d23",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cdf722",
   "metadata": {},
   "source": [
    "## Summary ðŸ“š\n",
    "- Neural networks are *much* more complicated on average than the algorithms we've seen thus far\n",
    "- Neural networks leverage **layers** to extract complex relationships\n",
    "- Feature extraction happens within the network itself, but your inputs may still require preprocessing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
