{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ed6989",
   "metadata": {},
   "source": [
    "# Deep Learning: More Convolutional Neural Networks\n",
    "\n",
    "Welcome back! Today’s small–group lecture explores **Convolutional Neural Networks (CNNs)**, building directly on what we've learned so far in deep learning.\n",
    "\n",
    "CNNs are the workhorse behind many modern computer vision systems — from image classification (e.g., recognizing cats vs. dogs) to object detection (e.g., detecting pedestrians in self-driving cars) and even style transfer.\n",
    "\n",
    "In a standard fully-connected neural network, every neuron in one layer connects to every neuron in the next. That works well for tabular data, but it ignores the fact that **images have spatial structure**: nearby pixels tend to be related. CNNs exploit this by using small filters (kernels) that slide over the image to detect local patterns like edges and textures.\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "- Understand the intuition behind CNNs and why they work so well for image data.\n",
    "- Build a multi-layer PyTorch CNN from scratch.\n",
    "- Explain the role of activation functions, batch normalization, and max pooling.\n",
    "- Train, validate, and analyze a CNN’s performance.\n",
    "- Understand core training concepts such as optimizers, loss functions, epochs, and learning rate.\n",
    "\n",
    "This lecture is designed to feel like a guided walkthrough — with explanations of not just **what** we do, but **why** we do it. As you go, try to connect the code to the mathematical ideas from lecture.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147a4157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.3.1.post100\n",
      "Torchvision version: 0.18.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n", 
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f632b4",
   "metadata": {},
   "source": [
    "### Note: Python must be 3.11.x\n",
    "\n",
    "If your kernel shows a value > 3.11.x you will need to downgrade. Please email the staff for help.\n",
    "\n",
    "**Why does this matter?** Deep learning frameworks like PyTorch provide precompiled binaries (called *wheels*) for specific Python versions and hardware setups. When Python introduces a new major/minor version (like 3.12), the internal binary interface (ABI) can change. If PyTorch hasn't released a compatible wheel yet, you may see mysterious runtime errors, crashes, or import failures.\n",
    "\n",
    "So as a rule of thumb:\n",
    "- Always check the PyTorch install page for supported Python versions.\n",
    "- If you see version mismatches, try creating a **conda or venv environment** with Python 3.11 specifically for deep learning work.\n",
    "\n",
    "In this class, we standardize on **Python 3.11.x** so the environment is reproducible and supportable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc40539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# DEVICE CONFIGURATION\n",
    "if torch.backends.mps.is_available():          # Apple Silicon (Metal Performance Shaders)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():                # NVIDIA GPU with CUDA\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")               # Fallback to CPU\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9dc771",
   "metadata": {},
   "source": [
    "For today's small group, we will walk through the process of setting up a convolutional neural network (\"CNN\" for short) using the `pytorch` package!\n",
    "\n",
    "CNNs shine when working with **structured grid data**, especially images — tasks like classification, segmentation, object detection, and more.\n",
    "\n",
    "Why? Because CNNs:\n",
    "- Capture local patterns (edges, textures) via **convolutions**.\n",
    "- Build hierarchical features (shapes → object parts → full objects) through **stacked layers**.\n",
    "- Use **shared weights**, meaning the same filter is applied across the whole image, which makes them parameter-efficient and **translation-equivariant** (shifting the image shifts the feature map in a predictable way).\n",
    "\n",
    "A quick mental picture:\n",
    "- Early layers learn to detect **edges** and simple textures.\n",
    "- Middle layers detect **parts** like eyes, wheels, or wings.\n",
    "- Later layers detect **semantic concepts** like \"dog\", \"car\", or \"bird\".\n",
    "\n",
    "Let’s load a dataset so we can see these ideas in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677b0b9",
   "metadata": {},
   "source": [
    "Recall from lecture that CNNs are generally used to process gridded data or images.\n",
    "\n",
    "Let's begin by loading one of the toy datasets included in `pytorch`: **CIFAR-10**.\n",
    "\n",
    "The dataset contains 60,000 small 32×32 color images belonging to 10 different classes:\n",
    "- airplane\n",
    "- automobile\n",
    "- bird\n",
    "- cat\n",
    "- deer\n",
    "- dog\n",
    "- frog\n",
    "- horse\n",
    "- ship\n",
    "- truck\n",
    "\n",
    "There are 50,000 training images and 10,000 test images. Each image has **3 color channels (RGB)** and a relatively low resolution (32×32), which makes it great for teaching and small experiments.\n",
    "\n",
    "We will also apply a **transform pipeline** to preprocess the images before feeding them into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dba6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init preprocessing for CIFAR-10 dataset (images are 32x32x3)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # ToTensor() converts images from [0, 255] uint8 to [0.0, 1.0] float32\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize each channel to roughly [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6085fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a204a",
   "metadata": {},
   "source": [
    "Great! We have image data now. But what does it look like?\n",
    "\n",
    "Visualizing your data is an essential first step — especially in computer vision. It's very easy to accidentally:\n",
    "- Misinterpret labels.\n",
    "- Apply the wrong preprocessing.\n",
    "- Feed the network images that are rotated, upside-down, or poorly scaled.\n",
    "\n",
    "Let's plot the different classes below using `matplotlib`. When teaching, this is a great moment to ask:\n",
    "- *What patterns do you notice?* (e.g., background colors, viewpoints, clutter)\n",
    "- *Which classes might be harder for the network? Why?* (e.g., cat vs. dog may be harder than airplane vs. frog)\n",
    "\n",
    "We won't fill in the plotting code here, but you are encouraged to:\n",
    "- Sample a batch from `train_loader`.\n",
    "- Show a grid of images.\n",
    "- Print the corresponding class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a300eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each class here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d308dd",
   "metadata": {},
   "source": [
    "## Building a CNN\n",
    "\n",
    "Staff:\n",
    "- Reference lecture: CNNs involve stacking multiple **layers**\n",
    "- The first part of a CNN involves stacking multiple layers of convolutional, activation, and maxpool layers \n",
    "- The example code below shows 3 of these stacks of layers!\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20250529121802516451/Convolutional-Neural-Network-in-Machine-Learning.webp\" width = \"500\">\n",
    "</p>\n",
    "\n",
    "Conceptually, our CNN will:\n",
    "1. Take a 3×32×32 image.\n",
    "2. Apply a series of convolution + nonlinearity + pooling operations that gradually reduce spatial size but increase the number of feature maps.\n",
    "3. Flatten the final feature maps into a vector.\n",
    "4. Feed that vector into a fully connected layer that outputs **logits** for the 10 CIFAR-10 classes.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Staff:\n",
    "- Please briefly review some of the common activation functions (there will be a table on this at the beginning of lecture)\n",
    "- Discuss 3 of the most common activation functions\n",
    "- Be sure to define what their names are in tensorflow\n",
    "\n",
    "### Expanded Notes for Staff\n",
    "- **ReLU** (Rectified Linear Unit): `f(x) = max(0, x)`\n",
    "  - Very simple, fast to compute.\n",
    "  - Avoids vanishing gradients on the positive side.\n",
    "  - TensorFlow: `tf.nn.relu` or `tf.keras.layers.ReLU()`.\n",
    "\n",
    "- **LeakyReLU**: `f(x) = x` if `x > 0`, and `αx` otherwise (α is small, like 0.01).\n",
    "  - Solves the \"dying ReLU\" problem by allowing a small negative gradient.\n",
    "  - TensorFlow: `tf.nn.leaky_relu`.\n",
    "\n",
    "- **Tanh**: squashes values to (-1, 1).\n",
    "  - Zero-centered, which can help optimization.\n",
    "  - Still suffers from saturation/vanishing gradients at large |x|.\n",
    "  - TensorFlow: `tf.nn.tanh` or `tf.keras.activations.tanh`.\n",
    "\n",
    "Explain *why* we use nonlinear activations: without them, a stack of layers collapses to a single linear transformation — so no matter how many layers you add, the overall function is still just a linear mapping. Nonlinear activations give the network the ability to approximate **complex nonlinear functions**.\n",
    "\n",
    "Please add comments to this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11039ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Input shape: (batch_size, 3, 32, 32)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),        # -> (batch_size, 32, 32, 32)\n",
    "            nn.BatchNorm2d(32),                                # normalize activations per channel\n",
    "            nn.ReLU(),                                         # nonlinear activation\n",
    "            nn.MaxPool2d(2)                                    # -> (batch_size, 32, 16, 16)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),       # -> (batch_size, 64, 16, 16)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                                    # -> (batch_size, 64, 8, 8)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),      # -> (batch_size, 128, 8, 8)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                                    # -> (batch_size, 128, 4, 4)\n",
    "        )\n",
    "        # Flattened feature vector size: 128 * 4 * 4 = 2048\n",
    "        self.fc = nn.Linear(128*4*4, 10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)  # flatten (batch_size, 128*4*4)\n",
    "        out = self.fc(out)               # output logits (batch_size, 10)\n",
    "        return out\n",
    "\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2320e",
   "metadata": {},
   "source": [
    "### Q: What do you think will happen to your CNN as you change the activation function?\n",
    "Feel free to try this by changing the activation layers in `CNN`!\n",
    "\n",
    "### A:\n",
    "Different activation functions drastically change how gradients behave and how expressive the model is.\n",
    "\n",
    "- **ReLU**:\n",
    "  - Pros: fast, tends to work well out-of-the-box, avoids vanishing gradients on the positive side.\n",
    "  - Cons: some neurons can \"die\" (output 0 for all future inputs) if weights push them too far into the negative region.\n",
    "\n",
    "- **Sigmoid**:\n",
    "  - Squashes values to (0, 1).\n",
    "  - Gradients get very small for large |x| (saturation), leading to **vanishing gradients** and slow learning.\n",
    "  - Often a poor choice for deep CNNs.\n",
    "\n",
    "- **Tanh**:\n",
    "  - Similar to sigmoid but output is centered at 0 in (-1, 1), which can help optimization somewhat.\n",
    "  - Still suffers from saturation and vanishing gradients.\n",
    "\n",
    "- **LeakyReLU**:\n",
    "  - Keeps a small negative slope for x < 0, so neurons never fully die.\n",
    "  - Often slightly more robust than plain ReLU.\n",
    "\n",
    "So, if you switch from ReLU to sigmoid or tanh, you might observe:\n",
    "- Slower learning.\n",
    "- Worse final accuracy.\n",
    "- Training that appears to “stall” early.\n",
    "\n",
    "If you switch from ReLU to LeakyReLU, you might get slightly smoother training and sometimes a small performance boost, especially in deeper networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf767de",
   "metadata": {},
   "source": [
    "----\n",
    "## Training a CNN\n",
    "Staff:\n",
    "- Please discuss the selection process for optimizer and loss inputs\n",
    "- Define what learning rate is\n",
    "- Give an overview of 2-3 common loss functions and their behavior\n",
    "\n",
    "### Expanded Lecture Notes\n",
    "\n",
    "Training a neural network is an instance of **iterative optimization**. The key components are:\n",
    "\n",
    "1. **Loss function (objective)**: tells us how bad our predictions are.\n",
    "2. **Optimizer**: decides how to update parameters to reduce the loss.\n",
    "3. **Learning rate**: controls how big each update step is.\n",
    "4. **Epochs and batches**: define how many passes we make over the dataset and how we break data into mini-batches.\n",
    "\n",
    "### Loss function\n",
    "- For multi-class classification like CIFAR-10, we treat the model’s output as **logits** (unnormalized scores) for each class and compare them to the correct class labels.\n",
    "- `nn.CrossEntropyLoss()` in PyTorch:\n",
    "  - Applies a softmax internally to convert logits to probabilities.\n",
    "  - Computes the negative log-likelihood of the correct class.\n",
    "  - Encourages the network to assign high probability to the correct label.\n",
    "\n",
    "Other loss functions (for context):\n",
    "- `nn.MSELoss()` (mean squared error): often used in regression, not ideal for classification because it doesn’t match the probabilistic structure of the problem.\n",
    "- `nn.BCEWithLogitsLoss()`: used for binary classification or multi-label classification (where each class is independently 0 or 1).\n",
    "\n",
    "### Optimizer\n",
    "- **SGD (Stochastic Gradient Descent)**:\n",
    "  - Update rule: `θ ← θ − η * ∇_θ L` (plus optional momentum).\n",
    "  - Simple and effective but can require careful tuning of the learning rate and momentum.\n",
    "\n",
    "- **Adam (Adaptive Moment Estimation)**:\n",
    "  - Keeps moving averages of both gradients and squared gradients.\n",
    "  - Adapts the step size for each parameter separately.\n",
    "  - Often converges faster with less manual tuning, making it a great default choice.\n",
    "\n",
    "### Learning rate\n",
    "- The learning rate `η` determines the **step size** in parameter space.\n",
    "- Too large: the loss can oscillate or blow up, because we overshoot the minimum.\n",
    "- Too small: the network learns extremely slowly and may appear stuck.\n",
    "- In practice, people often use **learning rate schedules** (decay over time) or **adaptive optimizers** like Adam.\n",
    "\n",
    "[This](https://www.geeksforgeeks.org/machine-learning/epoch-in-machine-learning/) reference will be useful to review the concept of epochs if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "910f2a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Loss and optimizer\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327ce76",
   "metadata": {},
   "source": [
    "### Q: What might happen if we changed our loss from __ to __ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217ce9c",
   "metadata": {},
   "source": [
    "### A:\n",
    "Let’s consider a few concrete changes:\n",
    "\n",
    "1. **CrossEntropyLoss → MSELoss** for CIFAR-10 classification:\n",
    "   - The model now tries to match one-hot vectors using squared error.\n",
    "   - The gradients can become very small in regions where they should be large (softmax-like structure is lost).\n",
    "   - Training tends to be slower and can converge to worse optima.\n",
    "\n",
    "2. **CrossEntropyLoss → BCEWithLogitsLoss** without changing labels:\n",
    "   - BCEWithLogitsLoss expects a different label format (typically floats 0/1 per class).\n",
    "   - You would be pretending the 10 classes are independent binary labels instead of mutually exclusive.\n",
    "   - This mismatch usually leads to incorrect behavior or outright shape errors.\n",
    "\n",
    "3. **CrossEntropyLoss → a custom margin-based loss**:\n",
    "   - Might encourage larger separation between the correct class logit and the others.\n",
    "   - Could improve robustness but would need careful design.\n",
    "\n",
    "In summary: **CrossEntropyLoss is the standard choice** for multi-class classification with mutually exclusive labels. Changing away from it without adjusting the problem formulation can significantly hurt performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38868e01",
   "metadata": {},
   "source": [
    "### Q: What happens if the `learning_rate` parameter is too high? Or too low?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c969a87",
   "metadata": {},
   "source": [
    "### A:\n",
    "- **Too high learning rate**:\n",
    "  - Loss may explode to very large values.\n",
    "  - The training curve may bounce around and never settle.\n",
    "  - We might see `nan` losses if the gradients become too large.\n",
    "\n",
    "- **Too low learning rate**:\n",
    "  - Training loss decreases very slowly.\n",
    "  - It may seem like the model \"is not learning\", but really it’s just taking tiny steps.\n",
    "  - You might stop early because it appears stuck near random-guess accuracy.\n",
    "\n",
    "In practice, you often try a few learning rates on a small number of epochs and see how the loss curve behaves. This is sometimes called a **learning rate sweep** or **learning rate range test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0828170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "# STAFF: Please add check for early stopping!!!\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    model.train()  # set model to training mode (enables dropout, batchnorm updates)\n",
    "    running_loss = 0.0\n",
    "    for i,(images, labels) in enumerate(tqdm(train_loader)):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()   # clear old gradients\n",
    "        loss.backward()         # compute new gradients via backprop\n",
    "        optimizer.step()        # update model parameters\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Note: here we print the loss from the last batch of the epoch\n",
    "    print(f'Loss: {loss.item():.4f} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303225d",
   "metadata": {},
   "source": [
    "### Q: What happens if you increase `epochs`? Will performance always improve as `epochs` increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faaaf70",
   "metadata": {},
   "source": [
    "### A:\n",
    "Performance improves early on, but after a certain number of epochs the model begins to **overfit**:\n",
    "- Training accuracy continues to increase.\n",
    "- Validation/test accuracy plateaus or starts to **decrease**.\n",
    "\n",
    "Intuitively, the model begins to memorize specific details and noise from the training set instead of learning general patterns that transfer to new data.\n",
    "\n",
    "Common strategies to combat overfitting include:\n",
    "- **Early stopping**: monitor validation loss/accuracy and stop when it stops improving.\n",
    "- **Regularization**: weight decay, dropout, data augmentation.\n",
    "- **More data**: often the best solution if available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186f05bd",
   "metadata": {},
   "source": [
    "----\n",
    "## Validating a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d4cdd",
   "metadata": {},
   "source": [
    "Staff: Please add comments/explanations as needed to this code!\n",
    "\n",
    "We now want to evaluate how well our trained model **generalizes** to unseen data. This is why we kept a separate **test set**.\n",
    "\n",
    "Key steps during evaluation:\n",
    "- Call `model.eval()` to set the model to **evaluation mode**.\n",
    "  - This tells layers like BatchNorm and Dropout to behave differently (e.g., BatchNorm uses running statistics instead of batch statistics, Dropout is disabled).\n",
    "- Use `torch.no_grad()` to avoid tracking gradient computations.\n",
    "  - This saves memory and speed, since we aren’t going to backpropagate.\n",
    "- Loop over the `test_loader`, compute predictions, and count how many are correct.\n",
    "\n",
    "The result is the **test accuracy**, which we treat as an estimate of how well our model will perform on new, real-world data from the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eb98ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "model.eval()  # set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # torch.max returns (max_value, index_of_max)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff15a83",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34171286",
   "metadata": {},
   "source": [
    "## Analyzing Performance\n",
    "- Staff: prompt some reflection about the plot below\n",
    "\n",
    "A single test accuracy number is useful, but it doesn’t tell the whole story. Often, we want to understand **how performance changes over time** as we train longer.\n",
    "\n",
    "Things to consider when looking at an accuracy-vs-epochs plot:\n",
    "- Does training accuracy keep going up while test accuracy plateaus? → Likely overfitting.\n",
    "- Is both training and test accuracy low and not improving? → Underfitting (model too simple, learning rate too low, not enough epochs, etc.).\n",
    "- Do we see noisy, jagged curves? → Maybe a high learning rate or small batch size.\n",
    "\n",
    "You can also:\n",
    "- Plot both **training loss** and **test loss** vs. epoch.\n",
    "- Check if there is a gap between training and test curves.\n",
    "- Use such plots to decide when to stop training or when to adjust hyperparameters.\n",
    "\n",
    "This is a great opportunity to discuss *hyperparameter tuning*, *model capacity*, and *training dynamics*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91954382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs. epochs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
