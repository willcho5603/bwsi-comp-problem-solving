{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96ed6989",
   "metadata": {},
   "source": [
    "# Deep Learning: More Convolutional Neural Networks\n",
    "\n",
    "Welcome back! Today’s small–group lecture explores **Convolutional Neural Networks (CNNs)**, building directly on what we've learned so far in deep learning.\n",
    "\n",
    "CNNs are the workhorse behind many modern computer vision systems — from image classification (e.g., recognizing cats vs. dogs) to object detection (e.g., detecting pedestrians in self-driving cars) and even style transfer.\n",
    "\n",
    "In a standard fully-connected neural network, every neuron in one layer connects to every neuron in the next. That works well for tabular data, but it ignores the fact that **images have spatial structure**: nearby pixels tend to be related. CNNs exploit this by using small filters (kernels) that slide over the image to detect local patterns like edges and textures.\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "- Understand the intuition behind CNNs and why they work so well for image data.\n",
    "- Build a multi-layer PyTorch CNN from scratch.\n",
    "- Explain the role of activation functions, batch normalization, and max pooling.\n",
    "- Train, validate, and analyze a CNN’s performance.\n",
    "- Understand core training concepts such as optimizers, loss functions, epochs, and learning rate.\n",
    "\n",
    "This lecture is designed to feel like a guided walkthrough — with explanations of not just **what** we do, but **why** we do it. As you go, try to connect the code to the mathematical ideas from lecture.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147a4157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.3.1.post100\n",
      "Torchvision version: 0.18.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n", 
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f632b4",
   "metadata": {},
   "source": [
    "### Note: Python must be 3.11.x\n",
    "\n",
    "If your kernel shows a value > 3.11.x you will need to downgrade. Please email the staff for help.\n",
    "\n",
    "**Why does this matter?** Deep learning frameworks like PyTorch provide precompiled binaries (called *wheels*) for specific Python versions and hardware setups. When Python introduces a new major/minor version (like 3.12), the internal binary interface (ABI) can change. If PyTorch hasn't released a compatible wheel yet, you may see mysterious runtime errors, crashes, or import failures.\n",
    "\n",
    "So as a rule of thumb:\n",
    "- Always check the PyTorch install page for supported Python versions.\n",
    "- If you see version mismatches, try creating a **conda or venv environment** with Python 3.11 specifically for deep learning work.\n",
    "\n",
    "In this class, we standardize on **Python 3.11.x** so the environment is reproducible and supportable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc40539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# DEVICE CONFIGURATION\n",
    "if torch.backends.mps.is_available():          # Apple Silicon (Metal Performance Shaders)\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():                # NVIDIA GPU with CUDA\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")               # Fallback to CPU\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9dc771",
   "metadata": {},
   "source": [
    "For today's small group, we will walk through the process of setting up a convolutional neural network (\"CNN\" for short) using the `pytorch` package!\n",
    "\n",
    "CNNs shine when working with **structured grid data**, especially images — tasks like classification, segmentation, object detection, and more.\n",
    "\n",
    "Why? Because CNNs:\n",
    "- Capture local patterns (edges, textures) via **convolutions**.\n",
    "- Build hierarchical features (shapes → object parts → full objects) through **stacked layers**.\n",
    "- Use **shared weights**, meaning the same filter is applied across the whole image, which makes them parameter-efficient and **translation-equivariant** (shifting the image shifts the feature map in a predictable way).\n",
    "\n",
    "A quick mental picture:\n",
    "- Early layers learn to detect **edges** and simple textures.\n",
    "- Middle layers detect **parts** like eyes, wheels, or wings.\n",
    "- Later layers detect **semantic concepts** like \"dog\", \"car\", or \"bird\".\n",
    "\n",
    "Let’s load a dataset so we can see these ideas in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677b0b9",
   "metadata": {},
   "source": [
    "Recall from lecture that CNNs are generally used to process gridded data or images.\n",
    "\n",
    "Let's begin by loading one of the toy datasets included in `pytorch`: **CIFAR-10**.\n",
    "\n",
    "The dataset contains 60,000 small 32×32 color images belonging to 10 different classes:\n",
    "- airplane\n",
    "- automobile\n",
    "- bird\n",
    "- cat\n",
    "- deer\n",
    "- dog\n",
    "- frog\n",
    "- horse\n",
    "- ship\n",
    "- truck\n",
    "\n",
    "There are 50,000 training images and 10,000 test images. Each image has **3 color channels (RGB)** and a relatively low resolution (32×32), which makes it great for teaching and small experiments.\n",
    "\n",
    "We will also apply a **transform pipeline** to preprocess the images before feeding them into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dba6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init preprocessing for CIFAR-10 dataset (images are 32x32x3)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # ToTensor() converts images from [0, 255] uint8 to [0.0, 1.0] float32\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize each channel to roughly [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6085fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                             download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                            download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a204a",
   "metadata": {},
   "source": [
    "Great! We have image data now. But what does it look like?\n",
    "\n",
    "Visualizing your data is an essential first step — especially in computer vision. It's very easy to accidentally:\n",
    "- Misinterpret labels.\n",
    "- Apply the wrong preprocessing.\n",
    "- Feed the network images that are rotated, upside-down, or poorly scaled.\n",
    "\n",
    "Let's plot the different classes below using `matplotlib`. When teaching, this is a great moment to ask:\n",
    "- *What patterns do you notice?* (e.g., background colors, viewpoints, clutter)\n",
    "- *Which classes might be harder for the network? Why?* (e.g., cat vs. dog may be harder than airplane vs. frog)\n",
    "\n",
    "We won't fill in the plotting code here, but you are encouraged to:\n",
    "- Sample a batch from `train_loader`.\n",
    "- Show a grid of images.\n",
    "- Print the corresponding class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a300eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot each class here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d308dd",
   "metadata": {},
   "source": [
    "## Building a CNN\n",
    "\n",
    "Staff:\n",
    "- Reference lecture: CNNs involve stacking multiple **layers**\n",
    "- The first part of a CNN involves stacking multiple layers of convolutional, activation, and maxpool layers \n",
    "- The example code below shows 3 of these stacks of layers!\n",
    "<p align=\"left\">\n",
    "    <img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20250529121802516451/Convolutional-Neural-Network-in-Machine-Learning.webp\" width = \"500\">\n",
    "</p>\n",
    "\n",
    "Conceptually, our CNN will:\n",
    "1. Take a 3×32×32 image.\n",
    "2. Apply a series of convolution + nonlinearity + pooling operations that gradually reduce spatial size but increase the number of feature maps.\n",
    "3. Flatten the final feature maps into a vector.\n",
    "4. Feed that vector into a fully connected layer that outputs **logits** for the 10 CIFAR-10 classes.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "Staff:\n",
    "- Please briefly review some of the common activation functions (there will be a table on this at the beginning of lecture)\n",
    "- Discuss 3 of the most common activation functions\n",
    "- Be sure to define what their names are in tensorflow\n",
    "\n",
    "### Expanded Notes for Staff\n",
    "- **ReLU** (Rectified Linear Unit): `f(x) = max(0, x)`\n",
    "  - Very simple, fast to compute.\n",
    "  - Avoids vanishing gradients on the positive side.\n",
    "  - TensorFlow: `tf.nn.relu` or `tf.keras.layers.ReLU()`.\n",
    "\n",
    "- **LeakyReLU**: `f(x) = x` if `x > 0`, and `αx` otherwise (α is small, like 0.01).\n",
    "  - Solves the \"dying ReLU\" problem by allowing a small negative gradient.\n",
    "  - TensorFlow: `tf.nn.leaky_relu`.\n",
    "\n",
    "- **Tanh**: squashes values to (-1, 1).\n",
    "  - Zero-centered, which can help optimization.\n",
    "  - Still suffers from saturation/vanishing gradients at large |x|.\n",
    "  - TensorFlow: `tf.nn.tanh` or `tf.keras.activations.tanh`.\n",
    "\n",
    "Explain *why* we use nonlinear activations: without them, a stack of layers collapses to a single linear transformation — so no matter how many layers you add, the overall function is still just a linear mapping. Nonlinear activations give the network the ability to approximate **complex nonlinear functions**.\n",
    "\n",
    "Please add comments to this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11039ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Input shape: (batch_size, 3, 32, 32)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),        # -> (batch_size, 32, 32, 32)\n",
    "            nn.BatchNorm2d(32),                                # normalize activations per channel\n",
    "            nn.ReLU(),                                         # nonlinear activation\n",
    "            nn.MaxPool2d(2)                                    # -> (batch_size, 32, 16, 16)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),       # -> (batch_size, 64, 16, 16)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                                    # -> (batch_size, 64, 8, 8)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),      # -> (batch_size, 128, 8, 8)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)                                    # -> (batch_size, 128, 4, 4)\n",
    "        )\n",
    "        # Flattened feature vector size: 128 * 4 * 4 = 2048\n",
    "        self.fc = nn.Linear(128*4*4, 10)  # CIFAR-10 has 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)  # flatten (batch_size, 128*4*4)\n",
    "        out = self.fc(out)               # output logits (batch_size, 10)\n",
    "        return out\n",
    "\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2320e",
   "metadata": {},
   "source": [
    "### Q: What do you think will happen to your CNN as you change the activation function?\n",
    "Feel free to try this by changing the activation layers in `CNN`!\n",
    "\n",
    "### A:\n",
    "Different activation functions drastically change how gradients behave and how expressive the model is.\n",
    "\n",
    "- **ReLU**:\n",
    "  - Pros: fast, tends to work well out-of-the-box, avoids vanishing gradients on the positive side.\n",
    "  - Cons: some neurons can \"die\" (output 0 for all future inputs) if weights push them too far into the negative region.\n",
    "\n",
    "- **Sigmoid**:\n",
    "  - Squashes values to (0, 1).\n",
    "  - Gradients get very small for large |x| (saturation), leading to **vanishing gradients** and slow learning.\n",
    "  - Often a poor choice for deep CNNs.\n",
    "\n",
    "- **Tanh**:\n",
    "  - Similar to sigmoid but output is centered at 0 in (-1, 1), which can help optimization somewhat.\n",
    "  - Still suffers from saturation and vanishing gradients.\n",
    "\n",
    "- **LeakyReLU**:\n",
    "  - Keeps a small negative slope for x < 0, so neurons never fully die.\n",
    "  - Often slightly more robust than plain ReLU.\n",
    "\n",
    "So, if you switch from ReLU to sigmoid or tanh, you might observe:\n",
    "- Slower learning.\n",
    "- Worse final accuracy.\n",
    "- Training that appears to “stall” early.\n",
    "\n",
    "If you switch from ReLU to LeakyReLU, you might get slightly smoother training and sometimes a small performance boost, especially in deeper networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf767de",
   "metadata": {},
   "source": [
    "----\n",
    "## Training a CNN\n",
    "Staff:\n",
    "- Please discuss the selection process for optimizer and loss inputs\n",
    "- Define what learning rate is\n",
    "- Give an overview of 2-3 common loss functions and their behavior\n",
    "\n",
    "
