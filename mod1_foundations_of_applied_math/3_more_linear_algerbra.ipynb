{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foundations of Applied Mathematics: More Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear algebra, we often study how matrices transform vectors. Imagine a matrix as a machine that stretches, shrinks, flips, or rotates space. Most vectors change direction when you apply a matrix to them — but some special vectors don’t. Those are called **eigenvectors**. If you have an eigenvector, then an **eigenvalue** tells you how much that eigenvector is stretched (or flipped) by the transformation.\n",
    "\n",
    "Mathematically, we say that an eigenvector $\\vec{v}$ that goes through some linear transformation, **T**, is scaled by a constant factor **x** when the linear trasnformation is applied to it:\n",
    "\n",
    "$$T\\vec{v} = x\\vec{v}$$\n",
    "\n",
    "So for the eigenvector $\\vec{v}$, the corresponding eigenvalue is that multiplying factor **x**. \n",
    "\n",
    "\n",
    "### Definitions:\n",
    "\n",
    "- **Eigenvector**: a vector that has its direction unchanged (or reversed) by a given linear transformation.\n",
    "- **Eigenvalue**: the factor by which an eigenvector is stretched or shrunk. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "\n",
    "Let's look at an example of a matrix and how we could find possible eigenvectors and eigenvalues. Say we have a matrix `M`:\n",
    "\n",
    "\n",
    "$$ M = \n",
    "\\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 3\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This matrix `M` scales the x-direction by 2 and the y-direction by 3.\n",
    "\n",
    "- Any vector along the x-axis (for example, [1, 0]) is just stretched by a factor of 2, so it is an eigenvector with an eigenvalue of 2.\n",
    "\n",
    "- Any vector along the y-axis (for example, [0, 1]) is stretched by 3, so it is an eigenvector with eigenvalue 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding:\n",
    "\n",
    "Let's look at how we can use Python to help us find these values. Take the matrix `A`:\n",
    "\n",
    "$$ A = \n",
    "\\begin{bmatrix}\n",
    "4 & 1 \\\\\n",
    "2 & 3\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can find the eigenvalues and eigenvectors of `A` using NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues: [5. 2.]\n",
      "Eigenvectors:\n",
      " [[ 0.70710678 -0.4472136 ]\n",
      " [ 0.70710678  0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[4, 1],\n",
    "              [2, 3]])\n",
    "\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "print(\"Eigenvalues:\", eigenvalues)\n",
    "print(\"Eigenvectors:\\n\", eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we'd interpret this output:\n",
    "\n",
    "- The matrix has two eigenvalues: 5 and 2.\n",
    "\n",
    "- Each eigenvalue has a corresponding eigenvector, which shows a direction that’s stretched (but NOT rotated, mirrored, etc):\n",
    "\n",
    "    - One direction (around [0.707, 0.707]) is stretched ×5\n",
    "\n",
    "    - The other (around [0.447, -0.894]) is stretched ×2\n",
    "\n",
    "So this transformation pulls strongly along one axis and less strongly along another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Find the eigenvalues and eigenvectors of the following matrix:\n",
    "\n",
    "$$ B = \n",
    "\\begin{bmatrix}\n",
    "3 & 1 \\\\\n",
    "0 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Do this by hand. Then, check your work using numpy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1\n",
    "Eigenvalues = \n",
    "\n",
    "Eigenvectors = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Consider the matrix:\n",
    "\n",
    "$$C = \\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "1 & 2 \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "a. Use NumPy to compute the eigenvalues and eigenvectors.\n",
    "\n",
    "b. Which directions (eigenvectors) are only stretched and not rotated by this matrix?\n",
    "\n",
    "c. Check that multiplying the matrix by each eigenvector equals the eigenvalue times the eigenvector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a: use NumPy to compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c: check that matrix * eigenvector = eigenvalue * eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- example: distance formula is just a vector norm\n",
    "- Ask the students to solve 1-2 cases with both analytical formula and then using numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Norms and Distance\n",
    "\n",
    "A **vector norm** measures the “length” of a vector.  \n",
    "The most common one is the **Euclidean norm** (or 2-norm):\n",
    "\n",
    "$$\n",
    "\\| \\mathbf{v} \\| = \\sqrt{v_1^2 + v_2^2 + \\dots + v_n^2}\n",
    "$$\n",
    "\n",
    "This is exactly the same as the **distance formula** that you've likely seen before:\n",
    "\n",
    "$$\n",
    "\\text{distance between points } P=(x_1, y_1) \\text{ and } Q=(x_2, y_2) \\text{ is } \n",
    "\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n",
    "$$\n",
    "\n",
    "So really:\n",
    "\n",
    "$$\n",
    "\\text{distance}(P, Q) = \\| \\mathbf{Q} - \\mathbf{P} \\|\n",
    "$$\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Distance between $(P = (1, 2))$ and $(Q = (4, 6))$:\n",
    "\n",
    "**Analytical:**\n",
    "\n",
    "$$\n",
    "\\sqrt{(4-1)^2 + (6-2)^2} = \\sqrt{3^2 + 4^2} = \\sqrt{9+16} = 5\n",
    "$$\n",
    "\n",
    "**Using vector norm:**\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = Q - P = [3, 4] \\quad\\Rightarrow\\quad \\| \\mathbf{v} \\| = \\sqrt{3^2 + 4^2} = 5\n",
    "$$\n",
    "\n",
    "**Using NumPy:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "P = np.array([1, 2])\n",
    "Q = np.array([4, 6])\n",
    "\n",
    "v = Q - P\n",
    "distance = np.linalg.norm(v)\n",
    "print(distance)  # Expected Output: 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Consider the points $A = (2, 6)$ and $B = (-1, 3)$. First, find the distance between these points by hand using the formula we saw. Then, check your answer by calculating the distance with NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Find distance by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Check your work with numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Use NumPy to calculate the distance between two 3-dimensional points:\n",
    "\n",
    "$C = (3, 7, 2)$ and $D = (8, -2, 2)$\n",
    "\n",
    "Hint: you can use `np.array` to store a 3-D point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate 3-D distance between C and D:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "**Singular Value Decomposition (SVD)** is a way to break down any matrix into three simpler matrices that capture its main patterns.  \n",
    "\n",
    "In simple terms, SVD is like taking a complicated transformation and seeing the main direction and degree in which it stretches or compresses space. Think of it as a way to find the most important building blocks of your data.  \n",
    "\n",
    "SVD is especially useful in computational analysis because it can help with:\n",
    "\n",
    "- **Image compression** – storing pictures in a smaller form while keeping most of the important features.  \n",
    "- **Principal Component Analysis (PCA)** – a method to find the main directions of variation in data (we will cover PCA later in the course).  \n",
    "\n",
    "**Further Reading:**\n",
    "\n",
    "- [SVD Definition on Wikipedia](https://en.wikipedia.org/wiki/Singular_value_decomposition)  \n",
    "- [Singular Value Decomposition on GeeksForGeeks](https://www.geeksforgeeks.org/machine-learning/singular-value-decomposition-svd/)  \n",
    "- [More Complex SVD visualization](https://www.youtube.com/watch?v=P5mlg91as1c)  \n",
    "\n",
    "In summary, SVD helps computers “understand” complex data by breaking it into simple, meaningful pieces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Least Squares\n",
    "\n",
    "**Linear Least Squares** is a method used to find the best-fitting line or curve for a set of data points.  \n",
    "\n",
    "#### Qualitative Idea\n",
    "Imagine you have several points on a graph and want to find a line that connects through all of them. Since it’s usually impossible for one line to pass exactly through every point, we choose a line that minimizes the total squared distances between the points and the line. These distances are called **residuals**.  \n",
    "\n",
    "Think of it as finding the line that “hugs” the points as closely as possible, and we judge how close our line is by calculating its distance from each of our points (square this distance and that is the residual). \n",
    "\n",
    "\n",
    "#### Symbolic Description\n",
    "\n",
    "For a set of data points $(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)$, we want a line:\n",
    "\n",
    "$$\n",
    "y = mx + b\n",
    "$$\n",
    "\n",
    "Define the **residuals**:\n",
    "\n",
    "$$\n",
    "r_i = y_i - (mx_i + b)\n",
    "$$\n",
    "\n",
    "Linear least squares chooses $(m)$ and $(b)$ to **minimize the sum of squared residuals**:\n",
    "\n",
    "$$\n",
    "S = \\sum_{i=1}^{n} r_i^2 = \\sum_{i=1}^{n} (y_i - (mx_i + b))^2\n",
    "$$\n",
    "\n",
    "The solution can also be written in **matrix form**:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = X \\mathbf{\\beta} + \\mathbf{r}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix}, \\quad\n",
    "\\mathbf{\\beta} = \\begin{bmatrix} b \\\\ m \\end{bmatrix}, \\quad\n",
    "\\mathbf{r} = \\text{residuals}\n",
    "$$\n",
    "\n",
    "The least squares solution is:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\beta} = (X^T X)^{-1} X^T \\mathbf{y}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Suppose we have three points: $(1,2), (2,3), (3,5)$ and want the best-fit line $y = mx + b$.\n",
    "\n",
    "**Step 1: Set up matrices**\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{bmatrix}, \\quad\n",
    "\\mathbf{y} = \\begin{bmatrix} 2 \\\\ 3 \\\\ 5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 2: Compute $\\beta = (X^T X)^{-1} X^T y$**\n",
    "\n",
    "$$\n",
    "X^T X = \\begin{bmatrix} 3 & 6 \\\\ 6 & 14 \\end{bmatrix}, \\quad\n",
    "X^T y = \\begin{bmatrix} 10 \\\\ 23 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{\\beta} = (X^T X)^{-1} X^T y = \\begin{bmatrix} 1 \\\\ 1.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "So the **best-fit line** is:\n",
    "\n",
    "$$\n",
    "\\boxed{y = 1.5x + 1}\n",
    "$$\n",
    "\n",
    "**Step 3: Computational Check with NumPy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33333333 1.5       ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1, 1],\n",
    "              [1, 2],\n",
    "              [1, 3]])\n",
    "y = np.array([2, 3, 5])\n",
    "\n",
    "beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(beta)  # Output: [1.  1.5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
