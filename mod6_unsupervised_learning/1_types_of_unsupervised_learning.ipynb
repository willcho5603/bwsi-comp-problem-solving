{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Types of Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is all about discovering hidden structure in unlabeled data. In this notebook, we'll explore several different clustering approaches and see how each one handles different shapes and patterns in data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, BisectingKMeans\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.metrics import silhouette_score, rand_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Unsupervised Learning?\n",
    "\n",
    "Unsupervised learning deals with data that has **no labels**, meaning the algorithm must discover structure on its own. Instead of learning by example (like supervised learning), unsupervised methods identify patterns, groupings, or relationships based only on the data itself.\n",
    "\n",
    "### Supervised vs Unsupervised: A Quick Recap\n",
    "- **Supervised Learning:** You have inputs *and* outputs — like a study guide with an answer key. The model learns to map features to known labels.\n",
    "- **Unsupervised Learning:** You only have inputs — like taking notes without knowing what the exam questions will be. The model looks for structure or grouping on its own.\n",
    "\n",
    "### Why \"Unsupervised\"?\n",
    "There’s no teacher, no correct answers provided ahead of time. The algorithm must uncover meaningful clusters or patterns entirely from the data structure.\n",
    "\n",
    "### Common Real-World Uses\n",
    "- Spotify: Groups similar songs into playlists\n",
    "- Retail: Identifying customer segments for marketing\n",
    "- Biology: Grouping gene expression profiles\n",
    "- News: Clustering articles by topics\n",
    "- Gaming: Detecting different play styles\n",
    "\n",
    "### Intuition Example\n",
    "Imagine sorting students into teams for an event without any categories given. You might group by behavior, interests, or who hangs out together — that's clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centroid-Based Clustering\n",
    "\n",
    "Centroid-based clustering groups data points according to their similarity to a central representative called a **centroid**. This centroid acts as the \"center of mass\" for the cluster.\n",
    "\n",
    "### The Big Idea\n",
    "Suppose you're organizing a pizza party. You want to place pizza stations (centroids) in a way that minimizes how far students must walk. Students naturally go to the station closest to them. Over time, stations would shift until each one is in the ideal location.\n",
    "\n",
    "Clustering works similarly:\n",
    "- Pick the number of clusters (number of pizza stations)\n",
    "- Assign each data point to its nearest centroid\n",
    "- Adjust centroids based on cluster members\n",
    "- Repeat until stable\n",
    "\n",
    "### Key Goals\n",
    "- **Minimize distances within clusters** (points in the same cluster should be close)\n",
    "- **Maximize distances between clusters** (clusters should be well-separated)\n",
    "\n",
    "This makes centroid-based clustering intuitive and computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the Clustering Process\n",
    "\n",
    "To get an intuition for how centroid-based methods work, let's walk through the process visually using K-Means as an example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_demo, _ = make_blobs(n_samples=150, centers=3, cluster_std=0.6, random_state=42)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "axes[0].scatter(X_demo[:, 0], X_demo[:, 1], c='gray', s=80, alpha=0.6, edgecolors='black')\n",
    "axes[0].set_title('Step 1: Raw Data (No Labels!)')\n",
    "\n",
    "initial_centroids = X_demo[np.random.choice(len(X_demo), 3, replace=False)]\n",
    "axes[1].scatter(X_demo[:, 0], X_demo[:, 1], c='gray', s=80, alpha=0.6, edgecolors='black')\n",
    "axes[1].scatter(initial_centroids[:, 0], initial_centroids[:, 1], c='red', s=400, marker='*', edgecolors='black')\n",
    "axes[1].set_title('Step 2: Random Initial Centroids')\n",
    "\n",
    "kmeans_demo = KMeans(n_clusters=3, init=initial_centroids, n_init=1, max_iter=1, random_state=42)\n",
    "labels_step3 = kmeans_demo.fit_predict(X_demo)\n",
    "axes[2].scatter(X_demo[:, 0], X_demo[:, 1], c=labels_step3, s=80, cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "axes[2].set_title('Step 3: Assign Points to Nearest Centroid')\n",
    "\n",
    "kmeans_final = KMeans(n_clusters=3, random_state=42)\n",
    "labels_final = kmeans_final.fit_predict(X_demo)\n",
    "axes[3].scatter(X_demo[:, 0], X_demo[:, 1], c=labels_final, s=80, cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "axes[3].scatter(kmeans_final.cluster_centers_[:, 0], kmeans_final.cluster_centers_[:, 1], c='red', s=400, marker='*', edgecolors='black')\n",
    "axes[3].set_title('Step 4: Updated Centroids (Converged)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "K-Means is the most widely used clustering algorithm because it's fast, intuitive, and works well when clusters are roughly spherical.\n",
    "\n",
    "### What Makes K-Means Unique?\n",
    "K-Means uses the **mean** (average position) of points in a cluster as the centroid.\n",
    "\n",
    "### The K-Means Algorithm\n",
    "1. Pick k\n",
    "2. Randomly place centroids\n",
    "3. Assign points\n",
    "4. Update centroids\n",
    "5. Repeat until stable"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Clustering Students by Study Habits"
   ]
  },

  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "n_students = 200\n",
    "X_students, true_labels = make_blobs(n_samples=n_students, centers=3, cluster_std=1.0, random_state=42)\n",
    "X_students[:, 0] = X_students[:, 0] * 5 + 20\n",
    "X_students[:, 1] = X_students[:, 1] * 10 + 75\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_students)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_students[:, 0], X_students[:, 1], c='gray', s=80, alpha=0.6, edgecolors='black')\n",
    "plt.title('Before K-Means')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_students[:, 0], X_students[:, 1], c=labels, cmap='viridis', s=80, alpha=0.6, edgecolors='black')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=500, marker='*', edgecolors='black')\n",
    "plt.title('After K-Means')\n",
    "\n",
    "plt.show()"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Understanding K-Means\n",
    "a. What happens if you increase k from 3 to 4?\n",
    "b. What does a cluster center (mean) represent?\n",
    "c. Name a scenario where K-Means might perform poorly."
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-Based Clustering: DBSCAN\n",
    "\n",
    "DBSCAN groups points by density rather than distance to centroids.\n",
    "\n",
    "### Core, Border, Noise Points\n",
    "- **Core:** many nearby points\n",
    "- **Border:** near a core but not dense\n",
    "- **Noise:** isolated"
   ]
  },

  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "kmeans_moons = KMeans(n_clusters=2, random_state=42)\n",
    "labels_kmeans = kmeans_moons.fit_predict(X_moons)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels_dbscan = dbscan.fit_predict(X_moons)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c='gray', s=80, edgecolors='black')\n",
    "axes[0].set_title('Original Data')\n",
    "\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_kmeans, cmap='viridis', s=80, edgecolors='black')\n",
    "axes[1].set_title('K-Means (Incorrect)')\n",
    "\n",
    "axes[2].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_dbscan, cmap='viridis', s=80, edgecolors='black')\n",
    "axes[2].set_title('DBSCAN (Correct)')\n",
    "\n",
    "plt.show()"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Understanding DBSCAN\n",
    "a. Why does DBSCAN work better than K-Means on moon shapes?\n",
    "b. What happens if eps is too small or too large?\n",
    "c. Give a real-world dataset where DBSCAN excels."
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectivity-Based Clustering (Hierarchical Clustering)\n",
    "\n",
    "Hierarchical clustering builds a **tree-like structure** called a dendrogram.\n",
    "\n",
    "### Approaches\n",
    "- **Agglomerative:** bottom-up merging\n",
    "- **Divisive:** top-down splitting"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "### Linkage Criteria\n",
    "- **Single linkage:** smallest distance\n",
    "- **Complete linkage:** largest distance\n",
    "- **Average linkage:** average pairwise distance\n",
    "- **Ward’s method:** minimizes variance\n",
    "\n",
    "Average linkage gives a balanced view of cluster similarity by looking at *all* pairwise distances rather than extremes."
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisive Clustering (Bisecting K-Means)"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Clustering\n",
    "\n",
    "### Silhouette Score\n",
    "Measures cohesion vs separation.\n",
    "\n",
    "### Rand Index\n",
    "Compares clustering to known labels."
   ]
  },

  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_eval, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.7, random_state=42)\n",
    "scores = []\n",
    "\n",
    "for k in range(2, 7):\n",
    "    kmeans_eval = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans_eval.fit_predict(X_eval)\n",
    "    scores.append(silhouette_score(X_eval, labels))\n",
    "\n",
    "plt.plot(range(2, 7), scores, marker='o')\n",
    "plt.title('Silhouette Score vs K')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Methods\n",
    "\n",
    "| Algorithm | Strengths | Weaknesses | Best Use |\n",
    "|----------|-----------|-------------|----------|\n",
    "| K-Means | Fast, easy | Bad for irregular shapes | Simple clusters |\n",
    "| DBSCAN | Arbitrary shapes, finds noise | Sensitive to eps | Irregular/noisy data |\n",
    "| Bisecting K-Means | Hierarchy, efficient | Still centroid-based | When hierarchy needed |"
   ]
  },

  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_compare, _ = make_circles(n_samples=400, factor=0.5, noise=0.05, random_state=42)\n",
    "\n",
    "labels_k = KMeans(n_clusters=2, random_state=42).fit_predict(X_compare)\n",
    "labels_d = DBSCAN(eps=0.2).fit_predict(X_compare)\n",
    "labels_b = BisectingKMeans(n_clusters=2, random_state=42).fit_predict(X_compare)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].scatter(X_compare[:, 0], X_compare[:, 1], c=labels_k)\n",
    "axes[0].set_title('K-Means')\n",
    "\n",
    "axes[1].scatter(X_compare[:, 0], X_compare[:, 1], c=labels_d)\n",
    "axes[1].set_title('DBSCAN')\n",
    "\n",
    "axes[2].scatter(X_compare[:, 0], X_compare[:, 1], c=labels_b)\n",
    "axes[2].set_title('Bisecting K-Means')\n",
    "\n",
    "plt.show()"
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "1. Which algorithm handles circles well?\n",
    "2. Which struggles?\n",
    "3. Why does DBSCAN succeed?\n",
    "4. How does noise affect each algorithm?"
   ]
  }
 ]
}
