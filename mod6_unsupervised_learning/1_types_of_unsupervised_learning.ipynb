{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Types of Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, BisectingKMeans\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.metrics import silhouette_score, rand_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## What is Unsupervised Learning?\n",
    "\n",
    "**Quick recap of supervised vs unsupervised:**\n",
    "- **Supervised Learning**: You have labeled data (like studying for a test with an answer key)\n",
    "  - Example: Classifying emails as spam or not spam when you already know which is which\n",
    "  \n",
    "- **Unsupervised Learning**: You have data but NO labels (like organizing your closet without instructions)\n",
    "  - Example: Grouping customers by shopping habits when you don't know the groups ahead of time\n",
    "\n",
    "### Why is it called \"Unsupervised\"?\n",
    "Because there's no \"teacher\" telling the algorithm what the right answer is! The algorithm must find patterns on its own.\n",
    "\n",
    "### Common Applications:\n",
    "Often thought of as **\"clustering\" algorithms** used for data exploration:\n",
    "- Spotify: Grouping songs into playlists by similarity\n",
    "- Amazon: Finding customer segments for targeted marketing\n",
    "- Biology: Classifying species based on genetic data\n",
    "- News: Organizing articles into topic categories\n",
    "- Gaming: Grouping players by playstyle\n",
    "\n",
    "### Fun Example:\n",
    "Imagine you're organizing a school party and need to group students into teams. You don't know who should be with whom, but you want:\n",
    "- Friends to be together\n",
    "- People with similar interests together\n",
    "- Balanced teams\n",
    "\n",
    "That's clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centroid_intro",
   "metadata": {},
   "source": [
    "## Centroid-Based Clustering\n",
    "\n",
    "Also known as **partitioning methods**.\n",
    "\n",
    "### The Big Idea:\n",
    "Imagine you're planning a pizza party and placing pizza stations (centroids) around the school so that:\n",
    "- Everyone can reach the closest station\n",
    "- The stations aren't too close to each other\n",
    "\n",
    "### Common Structure (All Centroid Methods):\n",
    "1. Initialize k centroids randomly\n",
    "2. Assign each point to the nearest centroid\n",
    "3. Update centroids based on current group members\n",
    "4. Repeat until centroids stop moving\n",
    "\n",
    "### Goals:\n",
    "- **Minimize within-cluster distance** (group members close together)\n",
    "- **Maximize between-cluster distance** (groups separated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization_process",
   "metadata": {},
   "source": [
    "### Visualization of the Clustering Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_process",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple dataset\n",
    "np.random.seed(42)\n",
    "X_demo, _ = make_blobs(n_samples=150, centers=3, cluster_std=0.6)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Step 1\n",
    "axes[0].scatter(X_demo[:, 0], X_demo[:, 1], c='gray', s=80, edgecolors='black', alpha=0.6)\n",
    "axes[0].set_title('Step 1: Raw Data')\n",
    "\n",
    "# Step 2\n",
    "initial_centroids = X_demo[np.random.choice(len(X_demo), 3, replace=False)]\n",
    "axes[1].scatter(X_demo[:, 0], X_demo[:, 1], c='gray', s=80, edgecolors='black', alpha=0.6)\n",
    "axes[1].scatter(initial_centroids[:, 0], initial_centroids[:, 1], c='red', marker='*', s=300, edgecolors='black')\n",
    "axes[1].set_title('Step 2: Random Centroids')\n",
    "\n",
    "# Step 3\n",
    "kmeans_demo = KMeans(n_clusters=3, init=initial_centroids, n_init=1, max_iter=1)\n",
    "labels_step3 = kmeans_demo.fit_predict(X_demo)\n",
    "axes[2].scatter(X_demo[:, 0], X_demo[:, 1], c=labels_step3, cmap='viridis', s=80, edgecolors='black')\n",
    "axes[2].set_title('Step 3: Assign Points')\n",
    "\n",
    "# Step 4\n",
    "kmeans_final = KMeans(n_clusters=3, random_state=42)\n",
    "labels_final = kmeans_final.fit_predict(X_demo)\n",
    "axes[3].scatter(X_demo[:, 0], X_demo[:, 1], c=labels_final, cmap='viridis', s=80, edgecolors='black')\n",
    "axes[3].scatter(kmeans_final.cluster_centers_[:, 0], kmeans_final.cluster_centers_[:, 1], c='red', marker='*', s=300)\n",
    "axes[3].set_title('Step 4: Final Centroids')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmeans_intro",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "### What Makes K-Means Special?\n",
    "It uses the **mean** of each cluster to define its center.\n",
    "\n",
    "### The Algorithm:\n",
    "1. Choose k\n",
    "2. Randomly place k centroids\n",
    "3. Assign points to nearest centroid\n",
    "4. Update centroid = mean of cluster\n",
    "5. Repeat until stable\n",
    "\n",
    "### Pros:\n",
    "- Fast and scalable\n",
    "- Easy to understand\n",
    "\n",
    "### Cons:\n",
    "- Must choose k\n",
    "- Sensitive to outliers\n",
    "- Assumes circular clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmeans_example",
   "metadata": {},
   "source": [
    "### Example: Clustering Students by Study Habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmeans_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create student study data\n",
    "np.random.seed(42)\n",
    "X_students, true_labels = make_blobs(n_samples=200, centers=3, cluster_std=1.0)\n",
    "\n",
    "# Scale features\n",
    "X_students[:, 0] = X_students[:, 0] * 5 + 20\n",
    "X_students[:, 1] = X_students[:, 1] * 10 + 75\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X_students)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_students[:, 0], X_students[:, 1], c='gray')\n",
    "plt.title('Before Clustering')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_students[:, 0], X_students[:, 1], c=labels, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='*', s=300)\n",
    "plt.title('After K-Means')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q_kmeans",
   "metadata": {},
   "source": [
    "### Q1: Understanding K-Means\n",
    "a. What happens if you change k from 3 to 4?\n",
    "b. What does the mean represent?\n",
    "c. When might K-Means fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbscan_intro",
   "metadata": {},
   "source": [
    "## Density-Based Clustering: DBSCAN\n",
    "\n",
    "### Key Concepts:\n",
    "- **Core points**: Points with many neighbors nearby\n",
    "- **Border points**: Points near edge of cluster\n",
    "- **Noise**: Outliers\n",
    "\n",
    "### Parameters:\n",
    "- **eps**: Neighborhood radius\n",
    "- **min_samples**: Minimum points to form dense region\n",
    "\n",
    "### Advantages:\n",
    "- No need to choose k\n",
    "- Finds arbitrary shapes\n",
    "- Identifies noise\n",
    "\n",
    "### Disadvantages:\n",
    "- Sensitive to eps\n",
    "- Struggles with varying densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbscan_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moon data\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.05)\n",
    "\n",
    "labels_kmeans = KMeans(n_clusters=2).fit_predict(X_moons)\n",
    "labels_dbscan = DBSCAN(eps=0.2, min_samples=5).fit_predict(X_moons)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c='gray')\n",
    "axes[0].set_title('Original Data')\n",
    "\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_kmeans)\n",
    "axes[1].set_title('K-Means')\n",
    "\n",
    "axes[2].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_dbscan)\n",
    "axes[2].set_title('DBSCAN')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hierarchical_intro",
   "metadata": {},
   "source": [
    "## Connectivity-Based Clustering (Hierarchical Clustering)\n",
    "\n",
    "### Big Idea:\n",
    "Builds nested groups (like a family tree)\n",
    "\n",
    "### Two types:\n",
    "- **Agglomerative** (bottom-up)\n",
    "- **Divisive** (top-down)\n",
    "\n",
    "### What is a Dendrogram?\n",
    "A tree showing how clusters merge as distance increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agglomerative_intro",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "### Algorithm:\n",
    "1. Start with each point as own cluster\n",
    "2. Merge closest pair\n",
    "3. Repeat\n",
    "4. Choose number of clusters by cutting tree\n",
    "\n",
    "### Linkage methods:\n",
    "- Single linkage\n",
    "- Complete linkage\n",
    "- Average linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divisive_intro2",
   "metadata": {},
   "source": [
    "## Divisive Clustering (Top-Down)\n",
    "- Start with one big cluster\n",
    "- Repeatedly split using Bisecting K-Means\n",
    "- Useful for hierarchical structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_methods",
   "metadata": {},
   "source": [
    "## Evaluation of Clustering Methods\n",
    "\n",
    "### Silhouette Score\n",
    "- Measures how close points are to their own cluster vs other clusters\n",
    "- Ranges from -1 to 1\n",
    "\n",
    "### Rand Index\n",
    "- Compares two clusterings\n",
    "- 1 = perfect match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eval, _ = make_blobs(n_samples=300, centers=3)\n",
    "scores = []\n",
    "for k in range(2,7):\n",
    "    labels = KMeans(n_clusters=k).fit_predict(X_eval)\n",
    "    scores.append(silhouette_score(X_eval, labels))\n",
    "\n",
    "plt.plot(range(2,7), scores, marker='o')\n",
    "plt.title('Silhouette Score by K')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_methods",
   "metadata": {},
   "source": [
    "## Comparison of Methods\n",
    "\n",
    "| Algorithm | Strengths | Weaknesses | Best Use |\n",
    "|----------|-----------|------------|----------|\n",
    "| K-Means | Fast, simple | Assumes spherical clusters | Balanced circular clusters |\n",
    "| DBSCAN | Finds irregular shapes, detects noise | Sensitive to eps | Arbitrary shapes, noise |\n",
    "| Bisecting K-Means | Hierarchical, efficient | Assumes spherical clusters | When structure matters |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_compare, _ = make_circles(n_samples=400, factor=0.5, noise=0.05)\n",
    "\n",
    "labels_k = KMeans(n_clusters=2).fit_predict(X_compare)\n",
    "labels_d = DBSCAN(eps=0.2, min_samples=5).fit_predict(X_compare)\n",
    "labels_b = BisectingKMeans(n_clusters=2).fit_predict(X_compare)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,5))\n",
    "axes[0].scatter(X_compare[:,0], X_compare[:,1], c=labels_k)\n",
    "axes[0].set_title('K-Means')\n",
    "\n",
    "axes[1].scatter(X_compare[:,0], X_compare[:,1], c=labels_d)\n",
    "axes[1].set_title('DBSCAN')\n",
    "\n",
    "axes[2].scatter(X_compare[:,0], X_compare[:,1], c=labels_b)\n",
    "axes[2].set_title('Bisecting K-Means')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discussion",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "1. Which algorithm handled circular shapes best?\n",
    "2. Which struggled and why?\n",
    "3. What happens when noise is added?\n",
    "4. Which algorithm seems most flexible?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
