{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Types of Unsupervised Learning\n",
    "\n",
    "Unsupervised learning is all about discovering hidden structure in unlabeled data. In this notebook, we'll explore several different clustering approaches and see how each one handles different shapes and patterns in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, BisectingKMeans\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.metrics import silhouette_score, rand_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## What is Unsupervised Learning?\n",
    "\n",
    "Unsupervised learning deals with data that has **no labels**, meaning the algorithm must discover structure on its own. Instead of learning by example (like supervised learning), unsupervised methods identify patterns, groupings, or relationships based only on the data itself.\n",
    "\n",
    "### Supervised vs Unsupervised: A Quick Recap\n",
    "- **Supervised Learning:** You have inputs *and* outputs â€” like a study guide with an answer key. The model learns to map features to known labels.\n",
    "- **Unsupervised Learning:** You only have inputs â€” like taking notes without knowing what the exam questions will be. The model looks for structure or grouping on its own.\n",
    "\n",
    "### Why \"Unsupervised\"?\n",
    "Thereâ€™s no teacher, no correct answers provided ahead of time. The algorithm must uncover meaningful clusters or patterns entirely from the data structure.\n",
    "\n",
    "### Common Real-World Uses\n",
    "- ðŸŽµ **Spotify:** Groups similar songs into playlists\n",
    "- ðŸ›’ **Retail:** Identifying customer segments for marketing\n",
    "- ðŸ§¬ **Biology:** Grouping gene expression profiles\n",
    "- ðŸ“° **News:** Clustering articles by topics\n",
    "- ðŸŽ® **Gaming:** Detecting different play styles\n",
    "\n",
    "### Intuition Example\n",
    "Imagine sorting students into teams for an event without any categories given. You might group by behavior, interests, or who hangs out together â€” that's clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centroid_intro",
   "metadata": {},
   "source": [
    "## Centroid-Based Clustering\n",
    "\n",
    "Centroid-based clustering groups data points according to their similarity to a central representative called a **centroid**. This centroid acts as the \"center of mass\" for the cluster.\n",
    "\n",
    "### The Big Idea\n",
    "Suppose you're organizing a pizza party. You want to place pizza stations (centroids) in a way that minimizes how far students must walk. Students naturally go to the station closest to them. Over time, stations would shift until each one is in the ideal location.\n",
    "\n",
    "Clustering works similarly:\n",
    "- Pick the number of clusters (number of pizza stations)\n",
    "- Assign each data point to its nearest centroid\n",
    "- Adjust centroids based on cluster members\n",
    "- Repeat until stable\n",
    "\n",
    "### Key Goals\n",
    "- **Minimize distances within clusters** (points in the same cluster should be close)\n",
    "- **Maximize distances between clusters** (clusters should be well-separated)\n",
    "\n",
    "This makes centroid-based clustering intuitive and computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization_process",
   "metadata": {},
   "source": [
    "### Visualization of the Clustering Process\n",
    "\n",
    "To get an intuition for how centroid-based methods work, let's walk through the process visually using K-Means as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_process",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple dataset\n",
    "np.random.seed(42)\n",
    "X_demo, _ = make_blobs(n_samples=150, centers=3, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Manually show clustering steps\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Step 1: Raw data\n",
    "axes[0].scatter(X_demo[:, 0], X_demo[:, 1], c='gray', s=80, alpha=0.6, edgecolors='black')\n",
    "axes[0].set_title('Step 1: Raw Data (No Labels!)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Step 2: Random initial centroids\n",
    "initial_centroids = X_demo[np.random.choice(len(X_demo), 3, replace=False)]\n",
    "axes[1].scatter(X_demo[:, 0], X_demo[:, 1], c='gray', s=80, alpha=0.6, edgecolors='black')\n",
    "axes[1].scatter(initial_centroids[:, 0], initial_centroids[:, 1], \n",
    "               c='red', s=400, marker='*', edgecolors='black', linewidth=2, label='Initial Centroids')\n",
    "axes[1].set_title('Step 2: Place Random Centroids', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Step 3: Assign points to nearest centroid\n",
    "kmeans_demo = KMeans(n_clusters=3, init=initial_centroids, n_init=1, max_iter=1, random_state=42)\n",
    "labels_step3 = kmeans_demo.fit_predict(X_demo)\n",
    "axes[2].scatter(X_demo[:, 0], X_demo[:, 1], c=labels_step3, s=80, cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "axes[2].scatter(initial_centroids[:, 0], initial_centroids[:, 1], \n",
    "               c='red', s=400, marker='*', edgecolors='black', linewidth=2, label='Centroids')\n",
    "axes[2].set_title('Step 3: Assign Points to Nearest Centroid', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Step 4: Update centroids and final result\n",
    "kmeans_final = KMeans(n_clusters=3, random_state=42)\n",
    "labels_final = kmeans_final.fit_predict(X_demo)\n",
    "axes[3].scatter(X_demo[:, 0], X_demo[:, 1], c=labels_final, s=80, cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "axes[3].scatter(kmeans_final.cluster_centers_[:, 0], kmeans_final.cluster_centers_[:, 1],\n",
    "               c='red', s=400, marker='*', edgecolors='black', linewidth=2, label='Final Centroids')\n",
    "axes[3].set_title('Step 4: Update Centroids & Repeat Until Convergence', fontsize=14, fontweight='bold')\n",
    "axes[3].set_xlabel('Feature 1')\n",
    "axes[3].set_ylabel('Feature 2')\n",
    "axes[3].legend()\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Notes\n",
    "print(\"ðŸŽ¯ Notice:\")\n",
    "print(\"  â€¢ Step 3 assignments may vary across runs due to centroid initialization.\")\n",
    "print(\"  â€¢ Different centroid-based algorithms compute cluster centers differently.\")\n",
    "print(\"  â€¢ The algorithm repeats assignment and update steps until centroids stabilize.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmeans_intro",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "K-Means is the most widely used clustering algorithm because it's fast, intuitive, and works well when clusters are roughly spherical.\n",
    "\n",
    "### What Makes K-Means Unique?\n",
    "K-Means uses the **mean (average position)** of points in a cluster as the centroid. This makes the algorithm computationally efficient and easy to interpret.\n",
    "\n",
    "### The Meaning of the Mean\n",
    "The mean is the center point that minimizes squared distance from all other points â€” much like finding the average location of a group of people standing on a field.\n",
    "\n",
    "### The K-Means Algorithm (Simplified)\n",
    "1. Pick the number of clusters (k)\n",
    "2. Randomly place k centroids\n",
    "3. Assign points to nearest centroid\n",
    "4. Recompute centroids as the mean of assigned points\n",
    "5. Repeat until convergence\n",
    "\n",
    "### Real-World Applications\n",
    "- Customer segmentation\n",
    "- Grouping similar Netflix movies\n",
    "- Image compression\n",
    "- Market segmentation\n",
    "\n",
    "### Strengths\n",
    "âœ” Fast and scalable\n",
    "âœ” Easy to explain\n",
    "âœ” Works well when clusters are compact and separated\n",
    "\n",
    "### Weaknesses\n",
    "âœ˜ Must choose k\n",
    "âœ˜ Sensitive to outliers\n",
    "âœ˜ Assumes clusters are roughly circular\n",
    "âœ˜ Struggles with differently shaped clusters"
   ]
  },  {
   "cell_type": "markdown",
   "id": "kmeans_example",
   "metadata": {},
   "source": [
    "### Example: Clustering Students by Study Habits\n",
    "\n",
    "In this example, we imagine a school where students differ in two measurable characteristics:\n",
    "- Hours spent studying per week\n",
    "- Average course grade\n",
    "\n",
    "We create synthetic data mimicking three natural groups of students:\n",
    "- Students who study a lot and receive high grades\n",
    "- Students who study moderately and perform adequately\n",
    "- Students who study very little and struggle\n",
    "\n",
    "K-Means can discover these groups even without labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmeans_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (CODE UNCHANGED)\n",
    "np.random.seed(42)\n",
    "n_students = 200\n",
    "X_students, true_labels = make_blobs(n_samples=n_students, centers=3, \n",
    "                                     cluster_std=1.0, random_state=42)\n",
    "X_students[:, 0] = X_students[:, 0] * 5 + 20\n",
    "X_students[:, 1] = X_students[:, 1] * 10 + 75\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_students)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_students[:, 0], X_students[:, 1], c='gray', s=80,\n",
    "           alpha=0.6, edgecolors='black')\n",
    "plt.xlabel('Study Hours per Week')\n",
    "plt.ylabel('Average Grade')\n",
    "plt.title('Before K-Means: All Students Together')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_students[:, 0], X_students[:, 1], c=labels, cmap='viridis',\n",
    "           s=80, alpha=0.6, edgecolors='black')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=500, marker='*',\n",
    "           edgecolors='black', linewidth=2)\n",
    "plt.xlabel('Study Hours per Week')\n",
    "plt.ylabel('Average Grade')\n",
    "plt.title('After K-Means: 3 Distinct Groups Found')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ“Š Cluster Profiles:\\n\")\n",
    "for i in range(3):\n",
    "    pts = X_students[labels == i]\n",
    "    print(f\"Cluster {i+1}: {len(pts)} students\")\n",
    "    print(f\"  Avg Study Hours: {pts[:,0].mean():.1f}\")\n",
    "    print(f\"  Avg Grade: {pts[:,1].mean():.1f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q_kmeans",
   "metadata": {},
   "source": [
    "### Q1: Understanding K-Means\n",
    "a. What happens if you increase k from 3 to 4? What new type of student group might appear?\n",
    "\n",
    "b. The cluster centers (red stars) represent the **mean position** of each cluster. In your own words, describe what that means conceptually.\n",
    "\n",
    "c. Name a scenario where K-Means might perform poorly, and explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a_kmeans",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbscan_intro",
   "metadata": {},
   "source": [
    "## Density-Based Clustering: DBSCAN\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) groups points by density instead of distance to a centroid. This gives DBSCAN the ability to find clusters of **arbitrary shapes**, identify **outliers**, and avoid specifying the number of clusters ahead of time.\n",
    "\n",
    "### Key Insights\n",
    "- Clusters are formed by areas where points are packed closely.\n",
    "- Sparse regions form boundaries between clusters.\n",
    "- Points in very low-density regions get labeled as **noise**.\n",
    "\n",
    "### Core, Border, and Noise Points\n",
    "- **Core Point:** Has enough neighbors within radius Îµ.\n",
    "- **Border Point:** Near a core point but not dense enough itself.\n",
    "- **Noise Point:** Belongs to no cluster (too isolated).\n",
    "\n",
    "### Hyperparameters: eps and min_samples\n",
    "- **eps:** Max distance for points to be considered neighbors.\n",
    "- **min_samples:** Minimum neighbors needed to form a dense region (often 5).\n",
    "\n",
    "If eps is too small â†’ many noise points.\n",
    "If eps is too large â†’ clusters blend together.\n",
    "\n",
    "### Why DBSCAN Is Useful\n",
    "- Finds non-circular clusters\n",
    "- Works with uneven cluster shapes\n",
    "- Automatically finds the number of clusters\n",
    "- Robust to outliers\n",
    "\n",
    "However, it struggles with highly variable density."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbscan_example",
   "metadata": {},
   "source": [
    "### Example: DBSCAN vs K-Means on Complex Shapes\n",
    "This example demonstrates that DBSCAN handles irregular shapes (like moon-shaped data) much better than K-Means, which assumes clusters are spherical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbscan_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (CODE UNCHANGED)\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "kmeans_moons = KMeans(n_clusters=2, random_state=42)\n",
    "labels_kmeans = kmeans_moons.fit_predict(X_moons)\n",
    "\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels_dbscan = dbscan.fit_predict(X_moons)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c='gray', s=80, edgecolors='black', alpha=0.6)\n",
    "axes[0].set_title('Original Moon-Shaped Data')\n",
    "\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_kmeans, cmap='viridis', s=80,\n",
    "               edgecolors='black', alpha=0.6)\n",
    "axes[1].set_title('K-Means (Incorrect)')\n",
    "\n",
    "axes[2].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_dbscan, cmap='viridis', s=80,\n",
    "               edgecolors='black', alpha=0.6)\n",
    "axes[2].set_title('DBSCAN (Correct)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"DBSCAN automatically detected:\")\n",
    "print(f\" - Clusters: {len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)}\")\n",
    "print(f\" - Outliers: {(labels_dbscan == -1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q_dbscan",
   "metadata": {},
   "source": [
    "### Q2: Understanding DBSCAN\n",
    "a. Why does DBSCAN work better than K-Means on moon-shaped data?\n",
    "\n",
    "b. What happens if eps is too small? Too large?\n",
    "\n",
    "c. Give a real-world dataset where DBSCAN would outperform K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a_dbscan",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hierarchical_intro",
   "metadata": {},
   "source": [
    "## Connectivity-Based Clustering (Hierarchical Clustering)\n",
    "\n",
    "Hierarchical clustering builds a **tree-like structure** of clusters, known as a dendrogram. This gives you both a high-level and detailed view of the grouping structure.\n",
    "\n",
    "Unlike K-Means or DBSCAN, hierarchical clustering shows you:\n",
    "- How clusters form progressively\n",
    "- How similar clusters are before merging\n",
    "- Where natural group boundaries exist\n",
    "\n",
    "### Two Approaches\n",
    "#### 1. Agglomerative (Bottom-Up)\n",
    "- Start with each point as its own cluster\n",
    "- Gradually merge the closest clusters\n",
    "- Continue until everything merges into one\n",
    "\n",
    "#### 2. Divisive (Top-Down)\n",
    "- Start with one big cluster\n",
    "- Recursively split into smaller clusters\n",
    "\n",
    "Agglomerative is more common and easier to compute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agglomerative_intro",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "Agglomerative clustering starts with each point on its own and repeatedly merges the closest pair of clusters.\n",
    "\n",
    "### Linkage Criteria\n",
    "- **Single linkage:** Minimum distance between clusters\n",
    "- **Complete linkage:** Maximum distance\n",
    "- **Average linkage:** Average distance\n",
    "- **Wardâ€™s method:** Minimizes variance within clusters\n",
    "\n",
    "Wardâ€™s method is most common since it creates compact, balanced clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divisive_intro",
   "metadata": {},
   "source": [
    "## Divisive Clustering (Top-Down)\n",
    "\n",
    "Divisive methods begin with one large cluster and iteratively split it apart.\n",
    "\n",
    "### Example: Bisecting K-Means\n",
    "- Runs K-Means with k=2 to split a cluster\n",
    "- Repeats splitting until the desired number of clusters is reached\n",
    "\n",
    "This approach is faster than full hierarchical clustering and gives you a hierarchy without building a full dendrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_methods",
   "metadata": {},
   "source": [
    "## Evaluation of Clustering Methods\n",
    "\n",
    "Since clustering has no labels, evaluation is trickier than supervised learning. We rely on metrics that measure **cohesion** (how tight clusters are) and **separation** (how far apart clusters are).\n",
    "\n",
    "### 1. Silhouette Score\n",
    "Measures how similar a point is to its own cluster vs other clusters.\n",
    "- **Near 1.0:** Great clustering\n",
    "- **Around 0:** Overlapping clusters\n",
    "- **Negative:** Wrong cluster assignment\n",
    "\n",
    "### 2. Rand Index\n",
    "Measures agreement between your clustering and a known ground truth (if available).\n",
    "- **1.0:** Identical\n",
    "- **0.0:** Completely different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (CODE UNCHANGED)\n",
    "np.random.seed(42)\n",
    "X_eval, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.7, random_state=42)\n",
    "scores = []\n",
    "\n",
    "for k in range(2, 7):\n",
    "    kmeans_eval = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans_eval.fit_predict(X_eval)\n",
    "    score = silhouette_score(X_eval, labels)\n",
    "    scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 7), scores, marker='o')\n",
    "plt.title('Silhouette Score vs. K')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_methods",
   "metadata": {},
   "source": [
    "## Comparison of Methods\n",
    "\n",
    "Below is a summary of the strengths and weaknesses of the three main clustering families:\n",
    "\n",
    "| Algorithm | Core Idea | Strengths | Weaknesses | Best Use Cases |\n",
    "|----------|-----------|-----------|-------------|----------------|\n",
    "| **K-Means** | Distance to centroids | Fast, simple | Poor on irregular shapes | Circular clusters, large datasets |\n",
    "| **DBSCAN** | Density of points | Finds arbitrary shapes, detects noise | Sensitive to eps | Geographic clusters, noisy data |\n",
    "| **Bisecting K-Means** | Recursive splitting | Hierarchical structure | Still spherical clusters | When hierarchy is wanted |\n",
    "\n",
    "We now visualize how each algorithm handles circular rings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (CODE UNCHANGED)\n",
    "X_compare, _ = make_circles(n_samples=400, factor=0.5, noise=0.05, random_state=42)\n",
    "kmeans_compare = KMeans(n_clusters=2, random_state=42)\n",
    "labels_k = kmeans_compare.fit_predict(X_compare)\n",
    "\n",
    "dbscan_compare = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels_d = dbscan_compare.fit_predict(X_compare)\n",
    "\n",
    "bkm_compare = BisectingKMeans(n_clusters=2, random_state=42)\n",
    "labels_b = bkm_compare.fit_predict(X_compare)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].scatter(X_compare[:, 0], X_compare[:, 1], c=labels_k, cmap='coolwarm', edgecolor='k')\n",
    "axes[0].set_title('K-Means')\n",
    "axes[1].scatter(X_compare[:, 0], X_compare[:, 1], c=labels_d, cmap='coolwarm', edgecolor='k')\n",
    "axes[1].set_title('DBSCAN')\n",
    "axes[2].scatter(X_compare[:, 0], X_compare[:, 1], c=labels_b, cmap='coolwarm', edgecolor='k')\n",
    "axes[2].set_title('Bisecting K-Means')\n",
    "plt.suptitle('Comparing Clustering Methods')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discussion",
   "metadata": {},
   "source": [
    "## Discussion ðŸ’¬\n",
    "\n",
    "Look at the plots above and think about:\n",
    "1. Which algorithm handles concentric circles correctly? (Hint: DBSCAN)\n",
    "2. Which struggles to separate inner vs outer rings? (Hint: K-Means)\n",
    "3. Why does DBSCAN succeed here? (Density-based logic)\n",
    "4. How would noise affect each algorithm differently?\n",
    "\n",
    "### Key Takeaway\n",
    "Different clustering algorithms excel on different types of data. There is **no single best method** â€” choosing the right approach depends heavily on the structure of your dataset."
   ]
  }
 ]
}
