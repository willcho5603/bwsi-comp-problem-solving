{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Unsupervised Learning: Types of Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, BisectingKMeans\n",
    "from sklearn.datasets import make_blobs, make_moons, make_circles\n",
    "from sklearn.metrics import silhouette_score, rand_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "## What is Unsupervised Learning?\n",
    "\n",
    "**Quick recap of supervised vs unsupervised:**\n",
    "- **Supervised Learning**: You have labeled data (like studying for a test with an answer key)\n",
    "  - Example: Classifying emails as spam or not spam when you already know which is which\n",
    "  \n",
    "- **Unsupervised Learning**: You have data but NO labels (like organizing your closet without instructions)\n",
    "  - Example: Grouping customers by shopping habits when you don't know the groups ahead of time\n",
    "\n",
    "### Why is it called \"Unsupervised\"?\n",
    "Because there's no \"teacher\" telling the algorithm what the right answer is! The algorithm must find patterns on its own.\n",
    "\n",
    "### Common Applications:\n",
    "Often thought of as **\"clustering\" algorithms** used for data exploration:\n",
    "- üéµ **Spotify**: Grouping songs into playlists by similarity\n",
    "- üõí **Amazon**: Finding customer segments for targeted marketing\n",
    "- üß¨ **Biology**: Classifying species based on genetic data\n",
    "- üì∞ **News**: Organizing articles into topic categories\n",
    "- üéÆ **Gaming**: Grouping players by playstyle\n",
    "\n",
    "### Fun Example:\n",
    "Imagine you're organizing a school party and need to group students into teams. You don't know who should be with whom, but you want:\n",
    "- Friends to be together\n",
    "- People with similar interests together\n",
    "- Balanced teams\n",
    "\n",
    "That's clustering! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centroid_intro",
   "metadata": {},
   "source": [
    "## Centroid-Based Clustering\n",
    "\n",
    "Also known as **partitioning methods**.\n",
    "\n",
    "### The Big Idea:\n",
    "Imagine you're planning a pizza party for your school. You want to place pizzas (centroids) in locations so that:\n",
    "- Everyone can easily reach the closest pizza\n",
    "- Pizza stations are spread out (not all in one corner!)\n",
    "\n",
    "**Centroid-based clustering works the same way!** Each cluster has a \"center point\" (centroid), and data points belong to whichever centroid is closest.\n",
    "\n",
    "### Common Structure (All Centroid Methods Follow This):\n",
    "\n",
    "1. **Initialize**: Start by randomly placing k centroids (like placing k pizza stations)\n",
    "2. **Assign**: Put each data point in the group of its nearest centroid (students go to closest pizza)\n",
    "3. **Update**: Move the centroid to the center of its group (relocate pizza based on where students are)\n",
    "4. **Repeat**: Keep doing steps 2-3 until centroids stop moving (pizza stations find optimal spots)\n",
    "\n",
    "### The Two Main Goals:\n",
    "‚úÖ **Minimize distances WITHIN clusters**: Keep group members close together\n",
    "\n",
    "‚úÖ **Maximize distances BETWEEN clusters**: Keep different groups far apart\n",
    "\n",
    "Think of it like:\n",
    "- **Within**: Your friend group should sit close together at lunch\n",
    "- **Between**: Different friend groups should have some space between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization_process",
   "metadata": {},
   "source": [
    "### Visualization of the Clustering Process\n",
    "\n",
    "Let's see how centroid-based clustering works step-by-step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz_process",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple dataset\n",
    "np.random.seed(42)\n",
    "X_demo, _ = make_blobs(n_samples=150, centers=3, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# Manually show clustering steps\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Step 1: Raw data\n",
    "axes[0].scatter(X_demo[:, 0], X_demo[:, 1], c='gray', s=80, alpha=0.6, edgecolors='black')\n",
    "axes[0].set_title('Step 1: Raw Data (No Labels!)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Step 2: Random initial centroids\n",
    "initial_centroids = X_demo[np.random.choice(len(X_demo), 3, replace=False)]\n",
    "axes[1].scatter(X_demo[:, 0], X_demo[:, 1], c='gray', s=80, alpha=0.6, edgecolors='black')\n",
    "axes[1].scatter(initial_centroids[:, 0], initial_centroids[:, 1], \n",
    "               c='red', s=400, marker='*', edgecolors='black', linewidth=2, label='Initial Centroids')\n",
    "axes[1].set_title('Step 2: Place Random Centroids', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Step 3: Assign points to nearest centroid\n",
    "kmeans_demo = KMeans(n_clusters=3, init=initial_centroids, n_init=1, max_iter=1, random_state=42)\n",
    "labels_step3 = kmeans_demo.fit_predict(X_demo)\n",
    "axes[2].scatter(X_demo[:, 0], X_demo[:, 1], c=labels_step3, s=80, \n",
    "               cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "axes[2].scatter(initial_centroids[:, 0], initial_centroids[:, 1], \n",
    "               c='red', s=400, marker='*', edgecolors='black', linewidth=2, label='Centroids')\n",
    "axes[2].set_title('Step 3: Assign Points to Nearest Centroid', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Step 4: Update centroids and final result\n",
    "kmeans_final = KMeans(n_clusters=3, random_state=42)\n",
    "labels_final = kmeans_final.fit_predict(X_demo)\n",
    "axes[3].scatter(X_demo[:, 0], X_demo[:, 1], c=labels_final, s=80, \n",
    "               cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "axes[3].scatter(kmeans_final.cluster_centers_[:, 0], kmeans_final.cluster_centers_[:, 1],\n",
    "               c='red', s=400, marker='*', edgecolors='black', linewidth=2, label='Final Centroids')\n",
    "axes[3].set_title('Step 4: Update Centroids & Repeat Until Convergence', fontsize=14, fontweight='bold')\n",
    "axes[3].set_xlabel('Feature 1')\n",
    "axes[3].set_ylabel('Feature 2')\n",
    "axes[3].legend()\n",
    "axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Notice:\")\n",
    "print(\"  ‚Ä¢ Step 3: The particular centroid points shown will vary depending on the algorithm\")\n",
    "print(\"  ‚Ä¢ Different algorithms use different ways to calculate the center (mean, median, etc.)\")\n",
    "print(\"  ‚Ä¢ The process repeats until centroids stop moving significantly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmeans_intro",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "The most popular centroid-based clustering algorithm!\n",
    "\n",
    "### What Makes K-Means Special?\n",
    "K-Means specifically uses the **mean (average)** of the cluster to define the centroid.\n",
    "\n",
    "### What is the Mean of a Cluster?\n",
    "The **mean** is just the average position of all points in the cluster!\n",
    "\n",
    "**Example**: Imagine 3 friends standing at these positions on a field:\n",
    "- Friend 1: position (2, 3)\n",
    "- Friend 2: position (4, 5)  \n",
    "- Friend 3: position (6, 7)\n",
    "\n",
    "The mean position (centroid) is:\n",
    "- X-coordinate: (2 + 4 + 6) / 3 = 4\n",
    "- Y-coordinate: (3 + 5 + 7) / 3 = 5\n",
    "- **Mean position**: (4, 5) - right in the middle!\n",
    "\n",
    "### The K-Means Algorithm:\n",
    "1. Choose k (number of clusters you want)\n",
    "2. Randomly place k centroids\n",
    "3. Assign each point to its nearest centroid\n",
    "4. Calculate the MEAN of each cluster and move the centroid there\n",
    "5. Repeat steps 3-4 until centroids stop moving\n",
    "\n",
    "### Real-World Example:\n",
    "**Netflix Movie Recommendations**: K-Means could group movies into clusters based on:\n",
    "- Genre, runtime, release year, ratings\n",
    "- Then recommend movies from the same cluster you like!\n",
    "\n",
    "### Pros of K-Means:\n",
    "‚úÖ **Fast**: Works well even with large datasets\n",
    "\n",
    "‚úÖ **Simple**: Easy to understand and implement\n",
    "\n",
    "‚úÖ **Scalable**: Can handle millions of data points\n",
    "\n",
    "‚úÖ **Converges**: Always finds a solution (though not always the best one)\n",
    "\n",
    "### Cons of K-Means:\n",
    "‚ùå **Must choose k ahead of time**: You need to know how many clusters you want\n",
    "\n",
    "‚ùå **Sensitive to initial centroids**: Different starting points can give different results\n",
    "\n",
    "‚ùå **Assumes spherical clusters**: Works best when clusters are round/circular\n",
    "\n",
    "‚ùå **Sensitive to outliers**: Extreme values can pull centroids away from the real center\n",
    "\n",
    "‚ùå **Struggles with different sizes/densities**: Doesn't work well when clusters have very different sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kmeans_example",
   "metadata": {},
   "source": [
    "### Example: Clustering Students by Study Habits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kmeans_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create student study data\n",
    "np.random.seed(42)\n",
    "n_students = 200\n",
    "\n",
    "# Generate 3 natural groups of students\n",
    "X_students, true_labels = make_blobs(n_samples=n_students, centers=3, \n",
    "                                     cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Scale to represent study hours and grade\n",
    "X_students[:, 0] = X_students[:, 0] * 5 + 20  # Study hours: 10-30 per week\n",
    "X_students[:, 1] = X_students[:, 1] * 10 + 75  # Grades: 50-100\n",
    "\n",
    "# Apply K-Means with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels = kmeans.fit_predict(X_students)\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Before clustering\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_students[:, 0], X_students[:, 1], c='gray', s=80, \n",
    "           alpha=0.6, edgecolors='black')\n",
    "plt.xlabel('Study Hours per Week', fontsize=12)\n",
    "plt.ylabel('Average Grade', fontsize=12)\n",
    "plt.title('Before K-Means: All Students Look the Same', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# After clustering\n",
    "plt.subplot(1, 2, 2)\n",
    "scatter = plt.scatter(X_students[:, 0], X_students[:, 1], c=labels, s=80,\n",
    "                     cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=500, marker='*',\n",
    "           edgecolors='black', linewidth=2, label='Centroids (Cluster Centers)')\n",
    "plt.xlabel('Study Hours per Week', fontsize=12)\n",
    "plt.ylabel('Average Grade', fontsize=12)\n",
    "plt.title('After K-Means: 3 Distinct Student Groups!', fontsize=13, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpret the clusters\n",
    "print(\"üìä Student Group Characteristics:\\n\")\n",
    "for i in range(3):\n",
    "    cluster_points = X_students[labels == i]\n",
    "    avg_hours = cluster_points[:, 0].mean()\n",
    "    avg_grade = cluster_points[:, 1].mean()\n",
    "    print(f\"Group {i+1}: {len(cluster_points)} students\")\n",
    "    print(f\"  Average study hours: {avg_hours:.1f} hours/week\")\n",
    "    print(f\"  Average grade: {avg_grade:.1f}%\")\n",
    "    if avg_hours < 18 and avg_grade < 70:\n",
    "        print(f\"  Profile: 'Struggling Students' - Need extra support!\")\n",
    "    elif avg_hours > 22 and avg_grade > 85:\n",
    "        print(f\"  Profile: 'High Achievers' - Doing great!\")\n",
    "    else:\n",
    "        print(f\"  Profile: 'Average Students' - Steady progress\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q_kmeans",
   "metadata": {},
   "source": [
    "### Q1: Understanding K-Means\n",
    "\n",
    "a. In the student clustering example above, what happens if you change k from 3 to 4? Try it!\n",
    "\n",
    "b. Looking at the cluster centers (red stars), explain in your own words what the \"mean\" represents.\n",
    "\n",
    "c. Can you think of a situation where K-Means might NOT work well? (Hint: Think about the cons we listed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a_kmeans",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbscan_intro",
   "metadata": {},
   "source": [
    "## Density-Based Clustering: DBSCAN\n",
    "\n",
    "**DBSCAN** = Density-Based Spatial Clustering of Applications with Noise\n",
    "\n",
    "### The Big Idea:\n",
    "Instead of using centroids, DBSCAN finds clusters based on how **densely packed** points are!\n",
    "\n",
    "**Real-World Analogy**: Think of a school cafeteria:\n",
    "- **Dense areas**: Tables where friend groups sit together (clusters!)\n",
    "- **Sparse areas**: Empty space between tables\n",
    "- **Outliers**: That one person sitting alone in the corner\n",
    "\n",
    "DBSCAN finds the \"crowded tables\" automatically!\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Core Points**: Points with many neighbors nearby (popular kids with lots of friends around)\n",
    "2. **Border Points**: Points on the edge of a cluster (sitting at the edge of a friend group)\n",
    "3. **Noise Points**: Points that don't belong to any cluster (outliers/loners)\n",
    "\n",
    "### Input Parameters:\n",
    "\n",
    "**eps (epsilon)**: The maximum distance between two points to be considered neighbors\n",
    "- Think: \"How close do people need to sit to be in the same friend group?\"\n",
    "- Small eps = tight clusters, Large eps = loose clusters\n",
    "\n",
    "**min_samples**: Minimum number of points to form a dense region\n",
    "- Think: \"How many people make a friend group vs just 2 people?\"\n",
    "- Common value: 5 (but depends on your data!)\n",
    "\n",
    "### The DBSCAN Algorithm (in words):\n",
    "\n",
    "1. **Pick a random unvisited point**\n",
    "2. **Find all neighbors within eps distance**\n",
    "3. **If there are at least min_samples neighbors:**\n",
    "   - Start a new cluster!\n",
    "   - Add all neighbors to the cluster\n",
    "   - Recursively check THEIR neighbors too (friend of a friend!)\n",
    "4. **If there aren't enough neighbors:**\n",
    "   - Mark as noise (for now - might be added to a cluster later)\n",
    "5. **Repeat** until all points are visited\n",
    "\n",
    "### Big Advantage Over K-Means:\n",
    "üéâ **Does NOT require choosing k ahead of time!** \n",
    "\n",
    "DBSCAN automatically figures out how many clusters exist based on the density of your data!\n",
    "\n",
    "### Other Advantages:\n",
    "‚úÖ **Finds arbitrarily shaped clusters**: Works with weird, non-circular shapes\n",
    "\n",
    "‚úÖ **Identifies outliers**: Labels noise points explicitly\n",
    "\n",
    "‚úÖ **No assumption about cluster shape**: Unlike K-Means\n",
    "\n",
    "### Disadvantages:\n",
    "‚ùå **Sensitive to parameters**: eps and min_samples need careful tuning\n",
    "\n",
    "‚ùå **Struggles with varying densities**: If some clusters are tight and others loose\n",
    "\n",
    "‚ùå **Slower than K-Means**: Especially on large datasets\n",
    "\n",
    "‚ùå **Hard to use with high-dimensional data**: Distance becomes less meaningful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbscan_example",
   "metadata": {},
   "source": [
    "### Example: DBSCAN vs K-Means on Complex Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbscan_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create moon-shaped data (K-Means will struggle!)\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=42)\n",
    "\n",
    "# Try K-Means\n",
    "kmeans_moons = KMeans(n_clusters=2, random_state=42)\n",
    "labels_kmeans = kmeans_moons.fit_predict(X_moons)\n",
    "\n",
    "# Try DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels_dbscan = dbscan.fit_predict(X_moons)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Original data\n",
    "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c='gray', s=80, \n",
    "               alpha=0.6, edgecolors='black')\n",
    "axes[0].set_title('Original Moon-Shaped Data', fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# K-Means result\n",
    "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_kmeans, s=80,\n",
    "               cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "axes[1].scatter(kmeans_moons.cluster_centers_[:, 0], kmeans_moons.cluster_centers_[:, 1],\n",
    "               c='red', s=500, marker='*', edgecolors='black', linewidth=2)\n",
    "axes[1].set_title('K-Means Result (Incorrect!)', fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# DBSCAN result  \n",
    "axes[2].scatter(X_moons[:, 0], X_moons[:, 1], c=labels_dbscan, s=80,\n",
    "               cmap='viridis', alpha=0.6, edgecolors='black')\n",
    "axes[2].set_title('DBSCAN Result (Correct!)', fontsize=13, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count outliers in DBSCAN\n",
    "n_outliers = (labels_dbscan == -1).sum()\n",
    "n_clusters = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
    "\n",
    "print(\"\\nüîç DBSCAN Results:\")\n",
    "print(f\"  ‚Ä¢ Found {n_clusters} clusters automatically (no need to specify k!)\")\n",
    "print(f\"  ‚Ä¢ Identified {n_outliers} outlier points\")\n",
    "print(\"\\nüí° Notice: K-Means tries to force circular clusters and fails!\")\n",
    "print(\"   DBSCAN correctly follows the crescent shapes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q_dbscan",
   "metadata": {},
   "source": [
    "### Q2: Understanding DBSCAN\n",
    "\n",
    "a. In your own words, explain why DBSCAN works better than K-Means for the moon-shaped data.\n",
    "\n",
    "b. What would happen if we set eps too small? Too large?\n",
    "\n",
    "c. Give a real-world example where DBSCAN would be better than K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a_dbscan",
   "metadata": {},
   "source": [
    "### A:\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hierarchical_intro",
   "metadata": {},
   "source": [
    "## Connectivity-Based Clustering (Hierarchical Clustering)\n",
    "\n",
    "Also known as **hierarchical clustering**.\n",
    "\n",
    "### The Big Idea:\n",
    "Build a **tree** of nested clusters, like a family tree or organization chart!\n",
    "\n",
    "**Real-World Analogy**: Think about classifying living things:\n",
    "- **Kingdom** ‚Üí Phylum ‚Üí Class ‚Üí Order ‚Üí Family ‚Üí Genus ‚Üí **Species**\n",
    "- Each level groups similar organisms\n",
    "- You can \"cut\" the tree at any level to get different numbers of groups!\n",
    "\n",
    "### What is a Dendrogram?\n",
    "\n",
    "A **dendrogram** is a tree diagram that shows how clusters are related!\n",
    "\n",
    "Think of it like a tournament bracket:\n",
    "- Bottom: Individual points (like individual players)\n",
    "- Moving up: Points merge into bigger groups (teams form)\n",
    "- Top: Everything in one giant cluster (championship winner)\n",
    "\n",
    "**How to read a dendrogram:**\n",
    "- **Height of connections**: Shows how similar/different clusters are\n",
    "  - Low connection = very similar\n",
    "  - High connection = very different\n",
    "- **Horizontal cuts**: Draw a line to decide how many clusters you want\n",
    "\n",
    "### Two Main Approaches:\n",
    "\n",
    "1. **Agglomerative** (bottom-up): Start with individuals, merge into groups\n",
    "   - Like forming friend groups: individuals ‚Üí pairs ‚Üí small groups ‚Üí large groups\n",
    "   \n",
    "2. **Divisive** (top-down): Start with everyone together, split into groups\n",
    "   - Like splitting a large class: everyone ‚Üí split ‚Üí split again ‚Üí individuals\n",
    "\n",
    "### Which is more common?\n",
    "**Agglomerative** is much more popular and faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agglomerative_intro",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering (Bottom-Up)\n",
    "\n",
    "### The Algorithm (in words):\n",
    "\n",
    "1. **Start**: Every point is its own cluster (everyone is alone)\n",
    "2. **Find**: The two CLOSEST clusters\n",
    "3. **Merge**: Combine them into one cluster (they become friends!)\n",
    "4. **Repeat**: Steps 2-3 until all points are in one big cluster\n",
    "5. **Cut the tree**: Choose how many clusters you want\n",
    "\n",
    "### Linkage Methods (How do we measure \"closest\"?):\n",
    "\n",
    "- **Single linkage**: Minimum distance between any two points\n",
    "  - Like: \"These groups are close if their nearest members are close\"\n",
    "  \n",
    "- **Complete linkage**: Maximum distance between any two points\n",
    "  - Like: \"These groups are close only if their farthest members are close\"\n",
    "  \n",
    "- **Average linkage**: Average distance between all pairs\n",
    "  - Like: \"On average, how close are these groups?\
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "divisive_intro",
   "metadata": {},
   "source": [
    "## Divisive Clustering (Top-Down)\n",
    "\n",
    "Also called **top-down clustering**, divisive methods start with everyone in a single large group and split them into smaller and smaller clusters.\n",
    "\n",
    "### The Big Idea:\n",
    "- Start with **one giant cluster** (like your entire school)\n",
    "- Then, **split it** into smaller and smaller groups based on differences (like separating by grade, then by class, then by friend group)\n",
    "\n",
    "This is the opposite of agglomerative clustering!\n",
    "\n",
    "### Example: Bisecting K-Means\n",
    "- A popular **divisive** method\n",
    "- Repeatedly applies K-Means with k=2 to split clusters into two smaller groups\n",
    "- Keeps splitting until the desired number of clusters is reached\n",
    "\n",
    "### Analogy:\n",
    "Think of it like cutting a pizza in half, then cutting each half again, and so on ‚Äî you start big and divide until you get the right number of slices.\n",
    "\n",
    "### When is Divisive Useful?\n",
    "- When you have **a large dataset** and want a **hierarchy of clusters**\n",
    "- When you want to see both broad and specific patterns (big groups and subgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_methods",
   "metadata": {},
   "source": [
    "## Evaluation of Clustering Methods\n",
    "\n",
    "Unlike supervised learning, we don‚Äôt have ‚Äúlabels‚Äù to check our answers ‚Äî so how do we know if our clustering was good?\n",
    "\n",
    "We use **evaluation metrics** that measure how well the clusters fit the data!\n",
    "\n",
    "### Common Metrics:\n",
    "\n",
    "#### 1. Silhouette Score\n",
    "Measures **how close** each point in a cluster is to points in its own cluster vs points in other clusters.\n",
    "\n",
    "üëâ **Formula (conceptually):**\n",
    "\\n\n",
    "Silhouette = (distance to nearest cluster - distance within own cluster) / max(between, within)\n",
    "\n",
    "üëâ **Range:** -1 to 1\n",
    "- Close to **1** ‚Üí points are well-clustered\n",
    "- Around **0** ‚Üí overlapping clusters\n",
    "- Below **0** ‚Üí wrong cluster assignment\n",
    "\n",
    "**Analogy:** Imagine you‚Äôre sitting with your best friends (high silhouette), vs sitting halfway between two groups (low silhouette). \n",
    "\n",
    "#### 2. Rand Index\n",
    "Measures how similar two clusterings are ‚Äî like comparing your algorithm‚Äôs results to a known classification (if available).\n",
    "\n",
    "- **1.0** means perfect agreement\n",
    "- **0.0** means random\n",
    "\n",
    "**Example:** If two teachers group students differently, the Rand Index measures how similar their groupings are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick demonstration of silhouette score\n",
    "np.random.seed(42)\n",
    "X_eval, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.7, random_state=42)\n",
    "\n",
    "# Try different numbers of clusters\n",
    "scores = []\n",
    "for k in range(2, 7):\n",
    "    kmeans_eval = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans_eval.fit_predict(X_eval)\n",
    "    score = silhouette_score(X_eval, labels)\n",
    "    scores.append(score)\n",
    "\n",
    "# Plot silhouette scores for different k values\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 7), scores, marker='o', linewidth=2)\n",
    "plt.title('Silhouette Score vs Number of Clusters (K)', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Higher Silhouette = Better clustering separation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare_methods",
   "metadata": {},
   "source": [
    "## Comparison of Methods\n",
    "\n",
    "Let‚Äôs compare the **three main clustering methods** we‚Äôve learned so far ‚Äî K-Means, DBSCAN, and Bisecting K-Means.\n",
    "\n",
    "| Algorithm | Core Idea | Strengths | Weaknesses | When to Use |\n",
    "|------------|------------|------------|-------------|--------------|\n",
    "| **K-Means** | Minimizes distances to centroids | Fast, simple, scales to large data | Must pick k, sensitive to outliers | When clusters are roughly circular and balanced |\n",
    "| **DBSCAN** | Groups by density | Finds irregular shapes, detects noise | Sensitive to eps/min_samples | When clusters have arbitrary shapes or you expect noise |\n",
    "| **Bisecting K-Means** | Repeatedly splits data top-down | Hierarchical insight, efficient | Still assumes spherical clusters | When you want hierarchical structure without dendrograms |\n",
    "\n",
    "### Visualization Example\n",
    "Let‚Äôs see all three in action using sample data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with circular and irregular patterns\n",
    "X_compare, _ = make_circles(n_samples=400, factor=0.5, noise=0.05, random_state=42)\n",
    "\n",
    "# Fit each method\n",
    "kmeans_compare = KMeans(n_clusters=2, random_state=42)\n",
    "labels_k = kmeans_compare.fit_predict(X_compare)\n",
    "\n",
    "dbscan_compare = DBSCAN(eps=0.2, min_samples=5)\n",
    "labels_d = dbscan_compare.fit_predict(X_compare)\n",
    "\n",
    "bkm_compare = BisectingKMeans(n_clusters=2, random_state=42)\n",
    "labels_b = bkm_compare.fit_predict(X_compare)\n",
    "\n",
    "# Visualize all three\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].scatter(X_compare[:, 0], X_compare[:, 1], c=labels_k, cmap='coolwarm', edgecolor='k')\n",
    "axes[0].set_title('K-Means')\n",
    "\n",
    "axes[1].scatter(X_compare[:, 0], X_compare[:, 1], c=labels_d, cmap='coolwarm', edgecolor='k')\n",
    "axes[1].set_title('DBSCAN')\n",
    "\n",
    "axes[2].scatter(X_compare[:, 0], X_compare[:, 1], c=labels_b, cmap='coolwarm', edgecolor='k')\n",
    "axes[2].set_title('Bisecting K-Means')\n",
    "\n",
    "plt.suptitle('Comparing Clustering Methods', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discussion",
   "metadata": {},
   "source": [
    "## Discussion üí¨\n",
    "\n",
    "Take a look at the plots above.\n",
    "\n",
    "1. Which algorithm best handled the circular pattern? (Hint: look at the edges!)\n",
    "2. Which one had trouble separating the inner and outer rings?\n",
    "3. Why do you think DBSCAN performed differently?\n",
    "4. How would these algorithms behave if we added random noise points?\n",
    "\n",
    "üëâ **Conclusion:** Different clustering algorithms are good at different types of data shapes. There‚Äôs no one-size-fits-all ‚Äî choosing the right algorithm depends on your dataset!"
   ]
  }
 ]
}
