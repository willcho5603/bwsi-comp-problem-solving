{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1fe2bbd1",
      "metadata": {},
      "source": [
        "# Supervised Learning: More Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b66ace1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca579a74",
      "metadata": {},
      "source": [
        "- Practice implementing classifier algorithms\n",
        "- Make or find 2 small datasets that are clean (or walk through/prompt how to clean the data)\n",
        "\n",
        "Please write each of the following problems:\n",
        "1. For the first dataset, setup an outline/skeleton code of how to assemble a basic LinearSVC\n",
        "2. For the second, have them compare the different boundaries found by using `LinearSVC`, `SVC(kernel='linear')`, and `SVC(kernel='rbf')`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a808192",
      "metadata": {},
      "source": [
        "## LinearSVC Problem\n",
        "- example/guidance up front\n",
        "- discuss the problem/dataset\n",
        "- justify the use of a LinearSVC\n",
        "- discuss results using a confusion matrix and classification report\n",
        "- need to define precision vs. recall"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b52d8581",
      "metadata": {},
      "source": [
        "## Comparing Kernels (20–30 Minute Guided Section)\n",
        "\n",
        "In this section, we'll explore how different **Support Vector Machine (SVM)** kernels separate data. Kernels allow SVMs to handle more complex decision boundaries — some straight lines, some curves.\n",
        "\n",
        "**Goal:** By the end of this exercise, you should be able to:\n",
        "- Understand what kernels do and why we might choose one over another.\n",
        "- Compare how linear and non-linear boundaries look.\n",
        "- Evaluate each model using confusion matrices and classification reports."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kernel-background",
      "metadata": {},
      "source": [
        "### Background\n",
        "A **kernel** is a mathematical function that transforms data into a higher-dimensional space where it’s easier to separate classes.\n",
        "\n",
        "| Kernel | Description | Typical Shape |\n",
        "|--|--|--|\n",
        "| Linear | Draws a straight line (or hyperplane) between classes | Straight boundary |\n",
        "| Polynomial | Curved boundaries that can adjust for complex patterns | Curved depending on degree |\n",
        "| RBF (Radial Basis Function) | Measures similarity using distance — good for circular or non-linear data | Smooth, circular boundaries |\n",
        "\n",
        "We’ll compare **LinearSVC**, **SVC(kernel='linear')**, and **SVC(kernel='rbf')** using the same dataset to see how the choice of kernel changes both the boundary and model accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kernel-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Create a simple non-linear dataset\n",
        "from sklearn.datasets import make_moons\n",
        "X, y = make_moons(n_samples=300, noise=0.25, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', edgecolor='k')\n",
        "plt.title('Our Nonlinear Dataset: Two Interlocking Moons')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q1-linear-svc",
      "metadata": {},
      "source": [
        "### Question 1: LinearSVC\n",
        "\n",
        "Let’s start simple with a **LinearSVC** model. This classifier tries to draw a straight line that separates the data.\n",
        "\n",
        "**Task:**\n",
        "- Fit a LinearSVC on `X_train` and `y_train`.\n",
        "- Predict on the test set.\n",
        "- Compute and print a confusion matrix and classification report.\n",
        "- Discuss what you observe — is a straight line enough here?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "linear-svc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LinearSVC Implementation\n",
        "linear_model = LinearSVC(random_state=42, max_iter=5000)\n",
        "linear_model.fit(X_train, y_train)\n",
        "y_pred_linear = linear_model.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix (LinearSVC):\\n\", confusion_matrix(y_test, y_pred_linear))\n",
        "print(\"\\nClassification Report (LinearSVC):\\n\", classification_report(y_test, y_pred_linear))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "precision-recall",
      "metadata": {},
      "source": [
        "### Precision vs. Recall\n",
        "Before we interpret the results, let’s define two key metrics:\n",
        "- **Precision**: Out of all the positive predictions the model made, how many were actually correct?\n",
        "  $$ Precision = \\frac{True\\ Positives}{True\\ Positives + False\\ Positives} $$\n",
        "- **Recall**: Out of all the actual positive examples, how many did the model correctly identify?\n",
        "  $$ Recall = \\frac{True\\ Positives}{True\\ Positives + False\\ Negatives} $$\n",
        "\n",
        "A perfect model has both precision and recall equal to 1.0, but in practice, improving one can lower the other."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q2-svc-linear",
      "metadata": {},
      "source": [
        "### Question 2: SVC (Linear Kernel)\n",
        "Now, let’s use `SVC(kernel='linear')` instead of LinearSVC. While they’re similar, `SVC` gives us a bit more flexibility and sometimes a cleaner margin.\n",
        "\n",
        "**Task:**\n",
        "- Train an `SVC(kernel='linear')` on the same data.\n",
        "- Compare its performance and boundary to the `LinearSVC` results.\n",
        "- Plot the decision boundary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svc-linear",
      "metadata": {},
      "outputs": [],
      "source": [
        "svc_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svc_linear.fit(X_train, y_train)\n",
        "y_pred_svc_linear = svc_linear.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix (SVC - Linear Kernel):\\n\", confusion_matrix(y_test, y_pred_svc_linear))\n",
        "print(\"\\nClassification Report (SVC - Linear Kernel):\\n\", classification_report(y_test, y_pred_svc_linear))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q3-svc-rbf",
      "metadata": {},
      "source": [
        "### Question 3: SVC (RBF Kernel)\n",
        "The **Radial Basis Function (RBF)** kernel can draw **curved** boundaries, which makes it powerful for non-linear data.\n",
        "\n",
        "**Task:**\n",
        "- Train `SVC(kernel='rbf')`.\n",
        "- Plot its boundary and evaluate its performance using a confusion matrix and classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svc-rbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "svc_rbf = SVC(kernel='rbf', gamma=0.7, C=1.0, random_state=42)\n",
        "svc_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svc_rbf.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix (SVC - RBF Kernel):\\n\", confusion_matrix(y_test, y_pred_rbf))\n",
        "print(\"\\nClassification Report (SVC - RBF Kernel):\\n\", classification_report(y_test, y_pred_rbf))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "plot-boundaries",
      "metadata": {},
      "source": [
        "### Visualizing All Kernels Together\n",
        "Now let’s compare all three classifiers visually.\n",
        "\n",
        "We’ll use **DecisionBoundaryDisplay** from scikit-learn to draw each SVM’s decision regions side by side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-comparison",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "models = {\n",
        "    'LinearSVC': linear_model,\n",
        "    'SVC (Linear Kernel)': svc_linear,\n",
        "    'SVC (RBF Kernel)': svc_rbf\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for ax, (name, model) in zip(axes, models.items()):\n",
        "    DecisionBoundaryDisplay.from_estimator(\n",
        "        model, X_train, response_method='predict', cmap='coolwarm', alpha=0.8, ax=ax\n",
        "    )\n",
        "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', edgecolor='k')\n",
        "    ax.set_title(name)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "plt.suptitle('Comparing Decision Boundaries of Different Kernels', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "compare-results",
      "metadata": {},
      "source": [
        "### Final Question: Interpreting the Results\n",
        "1. Which model best captured the moon-shaped data?\n",
        "2. How did the shape of each decision boundary affect accuracy?\n",
        "3. Why might simpler models (like LinearSVC) still be valuable even when their accuracy is lower?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
