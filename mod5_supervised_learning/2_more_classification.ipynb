{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1fe2bbd1",
      "metadata": {},
      "source": [
        "# Supervised Learning: More Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b66ace1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.datasets import make_classification, make_moons\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "overview",
      "metadata": {},
      "source": [
        "## Overview\n",
        "- Practice implementing classifier algorithms\n",
        "- Work with two small, clean datasets\n",
        "- Learn to assemble and evaluate a **LinearSVC** classifier\n",
        "- Compare results and decision boundaries from multiple **SVM kernels**\n",
        "\n",
        "### Roadmap\n",
        "1. **Dataset 1 – LinearSVC Skeleton**: Basic classification workflow and metrics.\n",
        "2. **Dataset 2 – Comparing Kernels**: Visualize how different SVM kernels (linear, RBF, polynomial) separate non-linear data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "linearSVC-section",
      "metadata": {},
      "source": [
        "## LinearSVC Problem (Dataset 1)\n",
        "We’ll start with data that’s **mostly linearly separable**.  The goal is to practice the standard steps of training, predicting, and evaluating using `LinearSVC`.\n",
        "\n",
        "### Why LinearSVC?\n",
        "A linear SVM is fast and effective when the classes can be divided by a straight line (or plane).  It maximizes the margin between classes and can still handle small amounts of overlap.\n",
        "\n",
        "### Evaluation Metrics\n",
        "- **Precision** = TP / (TP + FP) → Of the points predicted as positive, how many were correct?\n",
        "- **Recall** = TP / (TP + FN) → Of all true positives, how many did we catch?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataset1-skeleton",
      "metadata": {},
      "source": [
        "### Dataset 1 – Skeleton Code\n",
        "Follow this outline to build and evaluate your LinearSVC model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dataset1-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple linearly separable dataset\n",
        "X, y = make_classification(\n",
        "  n_samples=300,\n",
        "  n_features=2,\n",
        "  n_informative=2,\n",
        "  n_redundant=0,\n",
        "  class_sep=1.5,\n",
        "  random_state=42\n",
        ")\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Initialize model\n",
        "model = LinearSVC(random_state=42, max_iter=5000)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Confusion Matrix (LinearSVC):\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report (LinearSVC):\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, edgecolor='k')\n",
        "plt.title('Dataset 1 – Linearly Separable Example')\n",
        "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "transition",
      "metadata": {},
      "source": [
        "## Transition → Non-Linear Data\n",
        "Now that we’ve seen a linear example, let’s move to a dataset where a straight line doesn’t work so well. We’ll use the classic **interlocking moons** pattern to compare different SVM kernels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kernel-overview",
      "metadata": {},
      "source": [
        "## Comparing Kernels (Dataset 2)\n",
        "A **kernel** function lets SVMs separate data that is not linearly separable by mapping it into a higher-dimensional space.\n",
        "\n",
        "| Kernel | Shape of Boundary | When Useful |\n",
        "|--|--|--|\n",
        "| Linear | Straight | Data is mostly linearly separable |\n",
        "| Polynomial | Curved, medium complexity | Patterns with smooth curves |\n",
        "| RBF | Flexible, non-linear | Highly curved data like circles or moons |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "make-moons",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate non-linear dataset\n",
        "X2, y2 = make_moons(n_samples=300, noise=0.25, random_state=42)\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.3, random_state=42)\n",
        "\n",
        "plt.scatter(X2_train[:,0], X2_train[:,1], c=y2_train, cmap='coolwarm', edgecolor='k')\n",
        "plt.title('Dataset 2 – Interlocking Moons')\n",
        "plt.xlabel('Feature 1'); plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "linearSVC-moons-q",
      "metadata": {},
      "source": [
        "### Question A – `LinearSVC`\n",
        "Train a LinearSVC on this curved data. What do you predict will happen when a straight line tries to separate two moon-shaped classes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "linearSVC-moons-a",
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_moons = LinearSVC(random_state=42, max_iter=5000)\n",
        "linear_moons.fit(X2_train, y2_train)\n",
        "y2_pred_linear = linear_moons.predict(X2_test)\n",
        "\n",
        "print('Confusion Matrix (LinearSVC):\\n', confusion_matrix(y2_test, y2_pred_linear))\n",
        "print('\\nClassification Report (LinearSVC):\\n', classification_report(y2_test, y2_pred_linear))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "svc-linear-q",
      "metadata": {},
      "source": [
        "### Question B – `SVC(kernel='linear')`\n",
        "Fit a linear-kernel SVC and compare it to `LinearSVC`. Notice if the results are similar and why small differences might appear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svc-linear-a",
      "metadata": {},
      "outputs": [],
      "source": [
        "svc_lin = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svc_lin.fit(X2_train, y2_train)\n",
        "y2_pred_lin = svc_lin.predict(X2_test)\n",
        "\n",
        "print('Confusion Matrix (SVC Linear):\\n', confusion_matrix(y2_test, y2_pred_lin))\n",
        "print('\\nClassification Report (SVC Linear):\\n', classification_report(y2_test, y2_pred_lin))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "svc-rbf-q",
      "metadata": {},
      "source": [
        "### Question C – `SVC(kernel='rbf')`\n",
        "Try an RBF kernel that can form curved boundaries. Does this improve accuracy for the moon-shaped pattern?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svc-rbf-a",
      "metadata": {},
      "outputs": [],
      "source": [
        "svc_rbf = SVC(kernel='rbf', gamma=0.7, C=1.0, random_state=42)\n",
        "svc_rbf.fit(X2_train, y2_train)\n",
        "y2_pred_rbf = svc_rbf.predict(X2_test)\n",
        "\n",
        "print('Confusion Matrix (SVC RBF):\\n', confusion_matrix(y2_test, y2_pred_rbf))\n",
        "print('\\nClassification Report (SVC RBF):\\n', classification_report(y2_test, y2_pred_rbf))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "svc-poly-q",
      "metadata": {},
      "source": [
        "### Question D – `SVC(kernel='poly', degree=3)`\n",
        "Test a polynomial kernel. Does it capture the curvature well without over-fitting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svc-poly-a",
      "metadata": {},
      "outputs": [],
      "source": [
        "svc_poly = SVC(kernel='poly', degree=3, C=1.0, random_state=42)\n",
        "svc_poly.fit(X2_train, y2_train)\n",
        "y2_pred_poly = svc_poly.predict(X2_test)\n",
        "\n",
        "print('Confusion Matrix (SVC Polynomial):\\n', confusion_matrix(y2_test, y2_pred_poly))\n",
        "print('\\nClassification Report (SVC Polynomial):\\n', classification_report(y2_test, y2_pred_poly))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "visualize-kernels",
      "metadata": {},
      "source": [
        "## Visualize All Four Boundaries\n",
        "We’ll use `DecisionBoundaryDisplay` to compare decision regions for each kernel side by side."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-kernels",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {",
        "  'LinearSVC': linear_moons,",
        "  'SVC (Linear)': svc_lin,",
        "  'SVC (RBF)': svc_rbf,",
        "  'SVC (Polynomial)': svc_poly",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))",
        "axes = axes.ravel()\n",
        "\n",
        "for ax, (name, mdl) in zip(axes, models.items()):",
        "  DecisionBoundaryDisplay.from_estimator(",
        "    mdl, X2_train, response_method='predict', cmap='coolwarm', alpha=0.8, ax=ax",
        "  )",
        "  ax.scatter(X2_train[:,0], X2_train[:,1], c=y2_train, cmap='coolwarm', edgecolor='k')",
        "  ax.set_title(name)",
        "  ax.set_xlabel('Feature 1'); ax.set_ylabel('Feature 2')",
        "\n",
        "plt.suptitle('Decision Boundaries Across SVM Kernels', fontsize=14)",
        "plt.tight_layout()",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "reflection",
      "metadata": {},
      "source": [
        "## Reflection Questions\n",
        "1. Which kernel achieved the highest accuracy and why?\n",
        "2. How do the decision boundaries visually differ across kernels?\n",
        "3. Did you notice a trade-off between precision and recall for any model?\n",
        "4. If you had a spiral-shaped dataset, which kernel would you try first and why?\n",
        "\n",
        "Discuss briefly before we review as a class."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
