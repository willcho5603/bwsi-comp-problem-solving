{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1fe2bbd1",
      "metadata": {},
      "source": [
        "# Supervised Learning: More Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b66ace1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.inspection import DecisionBoundaryDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intro-context",
      "metadata": {},
      "source": [
        "In this lecture, we’ll learn how to compare **different SVM classifiers** and how kernel choice affects model performance.\n",
        "\n",
        "You’ll walk through several versions of Support Vector Machines (SVMs) and see how they classify a fun, slightly tricky dataset. By the end, you’ll know how to set up SVMs, interpret their confusion matrices, and visually compare their decision boundaries.\n",
        "\n",
        "**Learning goals:**\n",
        "- Understand what kernels do and when to use them.\n",
        "- Practice fitting LinearSVC and SVC models.\n",
        "- Interpret confusion matrices and classification reports.\n",
        "- Compare decision boundaries across kernel types."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dataset-intro",
      "metadata": {},
      "source": [
        "## Our Example Dataset: Interlocking Moons\n",
        "\n",
        "We'll use a simple two-class dataset shaped like interlocking moons. It’s great for visualizing how different classifiers draw boundaries.\n",
        "\n",
        "The points represent two classes that are not perfectly separable by a straight line — perfect for seeing how kernels handle non-linearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "make-data",
      "metadata": {},
      "outputs": [],
      "source": [
        "X, y = make_moons(n_samples=300, noise=0.25, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', edgecolor='k')\n",
        "plt.title('Training Data: Two Interlocking Moons')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "background-svm",
      "metadata": {},
      "source": [
        "## What Is an SVM?\n",
        "An **SVM (Support Vector Machine)** is a classifier that finds the best boundary (or **hyperplane**) to separate two classes.\n",
        "\n",
        "When the data isn’t perfectly separable, SVMs allow for some misclassifications by using a concept called **margin** — the distance between the boundary and the nearest data points (support vectors).\n",
        "\n",
        "Different **kernels** allow the SVM to fit different shapes of boundaries. Let’s explore them!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q1-linearsvc",
      "metadata": {},
      "source": [
        "### Question 1: LinearSVC\n",
        "\n",
        "Let’s start with the simplest version — **LinearSVC**. This model tries to draw a straight line that separates the two classes.\n",
        "\n",
        "**Steps:**\n",
        "1. Fit a `LinearSVC` to the training data.\n",
        "2. Predict on the test set.\n",
        "3. Evaluate performance using a confusion matrix and classification report.\n",
        "\n",
        "Think about whether a straight line will capture the moon shapes effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "linear-svc",
      "metadata": {},
      "outputs": [],
      "source": [
        "linear_model = LinearSVC(random_state=42, max_iter=5000)\n",
        "linear_model.fit(X_train, y_train)\n",
        "y_pred_linear = linear_model.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix (LinearSVC):\\n\", confusion_matrix(y_test, y_pred_linear))\n",
        "print(\"\\nClassification Report (LinearSVC):\\n\", classification_report(y_test, y_pred_linear))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "precision-recall-explain",
      "metadata": {},
      "source": [
        "#### Understanding the Metrics\n",
        "- **Precision** tells us how many of the predicted positives were actually correct.\n",
        "- **Recall** tells us how many of the actual positives were correctly identified.\n",
        "\n",
        "When your data is imbalanced (like predicting rare diseases), recall often matters more. For balanced datasets like this one, both matter equally.\n",
        "\n",
        "Think: what does your precision/recall tell you about the model’s ability to correctly label each moon?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q2-svc-linear",
      "metadata": {},
      "source": [
        "### Question 2: SVC with Linear Kernel\n",
        "\n",
        "Now, let’s try **`SVC(kernel='linear')`**. This model also draws a straight line, but can optimize differently than LinearSVC and gives us more control over regularization.\n",
        "\n",
        "We’ll check if there’s any performance difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svc-linear",
      "metadata": {},
      "outputs": [],
      "source": [
        "svc_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svc_linear.fit(X_train, y_train)\n",
        "y_pred_svc_linear = svc_linear.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix (SVC - Linear Kernel):\\n\", confusion_matrix(y_test, y_pred_svc_linear))\n",
        "print(\"\\nClassification Report (SVC - Linear Kernel):\\n\", classification_report(y_test, y_pred_svc_linear))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q3-svc-rbf",
      "metadata": {},
      "source": [
        "### Question 3: SVC with RBF Kernel\n",
        "\n",
        "The **Radial Basis Function (RBF)** kernel handles curved patterns by mapping data into higher dimensions. It’s great for cases where the relationship between classes isn’t linear — like our interlocking moons.\n",
        "\n",
        "Let’s train an `SVC(kernel='rbf')` and see how it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svc-rbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "svc_rbf = SVC(kernel='rbf', gamma=0.7, C=1.0, random_state=42)\n",
        "svc_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svc_rbf.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix (SVC - RBF Kernel):\\n\", confusion_matrix(y_test, y_pred_rbf))\n",
        "print(\"\\nClassification Report (SVC - RBF Kernel):\\n\", classification_report(y_test, y_pred_rbf))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q4-svc-poly",
      "metadata": {},
      "source": [
        "### Question 4: SVC with Polynomial Kernel\n",
        "\n",
        "The **Polynomial kernel** allows the model to create more complex, flexible curves. It’s useful when data has multiple subtle interactions.\n",
        "\n",
        "Let’s test `SVC(kernel='poly', degree=3)` and compare it to the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "svc-poly",
      "metadata": {},
      "outputs": [],
      "source": [
        "svc_poly = SVC(kernel='poly', degree=3, C=1.0, random_state=42)\n",
        "svc_poly.fit(X_train, y_train)\n",
        "y_pred_poly = svc_poly.predict(X_test)\n",
        "\n",
        "print(\"Confusion Matrix (SVC - Polynomial Kernel):\\n\", confusion_matrix(y_test, y_pred_poly))\n",
        "print(\"\\nClassification Report (SVC - Polynomial Kernel):\\n\", classification_report(y_test, y_pred_poly))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "visualize",
      "metadata": {},
      "source": [
        "## Visualizing Decision Boundaries\n",
        "Now that we’ve trained all four models, let’s plot their decision boundaries side by side. This helps us visually understand how each kernel “thinks” about the problem.\n",
        "\n",
        "Notice how the linear models will likely form a straight separation, while RBF and polynomial create curves around the moon shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "plot-all",
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    'LinearSVC': linear_model,\n",
        "    'SVC (Linear)': svc_linear,\n",
        "    'SVC (RBF)': svc_rbf,\n",
        "    'SVC (Polynomial)': svc_poly\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for ax, (name, model) in zip(axes, models.items()):\n",
        "    DecisionBoundaryDisplay.from_estimator(\n",
        "        model, X_train, response_method='predict', cmap='coolwarm', alpha=0.8, ax=ax\n",
        "    )\n",
        "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='coolwarm', edgecolor='k')\n",
        "    ax.set_title(name)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "plt.suptitle('Comparing Decision Boundaries Across Kernels', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final-reflection",
      "metadata": {},
      "source": [
        "### Reflection Question\n",
        "Take a few minutes to discuss with your group:\n",
        "\n",
        "1. Which kernel produced the most accurate model? Why?\n",
        "2. How do the shapes of the decision boundaries differ across kernels?\n",
        "3. Which kernel might you choose if your data looked more like a spiral, or like well-separated lines?\n",
        "4. Do simpler models ever have advantages even if they perform slightly worse on accuracy?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
